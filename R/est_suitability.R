#' Estimate Environmental Suitability (psi) for Cholera Transmission
#'
#' This function loads climate data, ENSO data, and weekly cholera cases data to estimate the environmental
#' suitability for cholera transmission based on various environmental factors. It uses lagged climate
#' covariates, scales them, and then fits an LSTM-based RNN model to predict cholera outbreaks. The model
#' is trained using past cholera case data and climate conditions, and predictions are made for environmental
#' suitability (psi) based on the climate covariates.
#'
#' @param PATHS A list containing paths where the data is stored. Typically generated by the `get_paths()` function and should include:
#' \itemize{
#'   \item \strong{DATA_CLIMATE}: Path to the directory where processed climate data is stored.
#'   \item \strong{DATA_ENSO}: Path to the directory where ENSO data is stored.
#'   \item \strong{DATA_WHO_WEEKLY}: Path to the directory containing processed weekly cholera cases data.
#'   \item \strong{MODEL_INPUT}: Path to save the processed model inputs and outputs.
#'   \item \strong{DOCS_FIGURES}: Path to save the generated plots.
#' }
#'
#' @return This function processes climate and cholera case data, fits an LSTM model, makes predictions on
#'         environmental suitability (psi), and saves both the predictions and covariate data. It also
#'         generates a plot showing the model fit (accuracy and loss over training epochs) and saves it to a
#'         specified directory.
#'
#' @details
#' The function performs the following steps:
#' \itemize{
#'   \item Loads processed climate and cholera data.
#'   \item Creates lagged versions of relevant climate variables (e.g., temperature, precipitation, ENSO data).
#'   \item Scales the climate covariates.
#'   \item Splits the data into training and test sets.
#'   \item Builds an LSTM-based recurrent neural network (RNN) model for predicting cholera outbreaks.
#'   \item Trains the model on the training set and evaluates its performance on the test set.
#'   \item Makes predictions on both the test set and the full dataset.
#'   \item Saves the predictions, covariate data, and model fit plots to the specified directories.
#' }
#'
#' @importFrom arrow read_parquet
#' @importFrom utils read.csv
#' @importFrom glue glue
#' @importFrom dplyr left_join select mutate filter group_by summarize ungroup
#' @importFrom tidyr pivot_wider complete.cases
#' @importFrom keras keras_model_sequential layer_lstm layer_dropout layer_dense compile fit evaluate predict
#' @importFrom ggplot2 ggplot geom_rect geom_bar geom_line facet_wrap labs scale_y_continuous scale_x_date theme_minimal theme element_text element_line element_blank annotate
#' @importFrom patchwork plot_layout
#' @importFrom scales scale_color_manual
#' @importFrom zoo rollmean
#' @importFrom lubridate today
#' @importFrom tensorflow keras
#' @export
#'
#' @examples
#' \dontrun{
#' # Assuming PATHS is defined and points to the correct data directories:
#' est_suitability(PATHS)
#' }
#'
#' @note
#' The LSTM model uses lagged climate variables to predict cholera suitability. The model's predictions are saved as
#' a CSV file and a plot showing the model fit (accuracy and loss) is generated.
#'
#' @seealso
#' \code{\link[keras]{layer_lstm}}, \code{\link[keras]{fit}}, \code{\link[ggplot2]{ggplot}}
#'

est_suitability <- function(PATHS, include_lagged_covariates=FALSE) {

     requireNamespace('keras3')
     requireNamespace('tensorflow')
     requireNamespace('tidyverse')

     message("Loading merged suitability data...")
     path <- file.path(PATHS$DATA_WHO_WEEKLY, 'cholera_country_weekly_suitability_data.csv')
     d_all <- read.csv(path, stringsAsFactors = FALSE)
     d_all$date_start <- as.Date(d_all$date_start)
     d_all$date_stop <- as.Date(d_all$date_stop)

     path <- file.path(PATHS$DATA_ELEVATION, 'country_elevation_mean.csv')
     elevation_data <- read.csv(path, stringsAsFactors = FALSE)
     d_all <- merge(d_all, elevation_data[c("iso_code", "elevation")], by="iso_code")

     message("Adding covariates...")
     covariates <- c(
          "temperature_2m_mean", "temperature_2m_max", "temperature_2m_min",
          "wind_speed_10m_mean", "wind_speed_10m_max", "cloud_cover_mean",
          "shortwave_radiation_sum", "relative_humidity_2m_mean",
          "relative_humidity_2m_max", "relative_humidity_2m_min",
          "dew_point_2m_mean", "dew_point_2m_min", "dew_point_2m_max",
          "precipitation_sum", "snowfall_sum", "pressure_msl_mean",
          "soil_moisture_0_to_10cm_mean", "et0_fao_evapotranspiration_sum",
          "DMI", "ENSO3", "ENSO34", "ENSO4", "elevation"
     )

     X_all <- d_all[, colnames(d_all) %in% covariates]
     sel <- complete.cases(X_all)
     d_all <- d_all[sel,]
     X_all <- X_all[sel,]

     d <- d_all[!is.na(d_all$cases_binary),]
     y <- d$cases_binary
     X <- d[,colnames(d_all) %in% covariates]

     sel <- complete.cases(X)
     d <- d[sel,]
     y <- y[sel]
     X <- X[sel,]

     # Step 3: Standardize the features (covariates)
     if (include_lagged_covariates) {

          X_all_lagged <- MOSAIC::make_lagged_data(X_all, lags=1:3)
          X_lagged <- MOSAIC::make_lagged_data(X, lags=1:3)

          X_all_scaled <- scale(X_all_lagged)
          X_scaled <- scale(X_lagged)


     } else {

          X_all_scaled <- scale(X_all)
          X_scaled <- scale(X)

     }

     # Step 4: Reshape the data for RNN
     timesteps <- 2  # If using 2 time steps
     n_features <- ncol(X_scaled)  # Number of features

     # Create a 3D array for LSTM: (samples, timesteps, features)
     X_all_reshaped <- array(X_all_scaled, dim = c(nrow(X_all_scaled), timesteps, n_features))
     X_reshaped <- array(X_scaled, dim = c(nrow(X_scaled), timesteps, n_features))

     # Step 5: Split the data into training and testing sets
     set.seed(99)
     train_indices <- sample(1:nrow(X_reshaped), 0.8 * nrow(X_reshaped))
     X_train <- X_reshaped[train_indices, , ]
     y_train <- y[train_indices]
     X_test <- X_reshaped[-train_indices, , ]
     y_test <- y[-train_indices]

     # Reshape target to 2D: (samples, 1)
     y_train_array <- array(y_train, dim = c(length(y_train), 1))
     y_test_array <- array(y_test, dim = c(length(y_test), 1))

     # Display total number of samples and number of positive samples in training and test sets
     cat('Number of samples in training set:', dim(X_train)[1], '\n')
     cat('Number of positive samples in training set:', sum(y_train == 1), '\n')
     cat('Number of samples in test set:', dim(X_test)[1], '\n')
     cat('Number of positive samples in test set:', sum(y_test == 1), '\n')

     # Step 6: Build and Compile the LSTM model
     model <- keras_model_sequential() %>%
          layer_lstm(units = 200, input_shape = c(timesteps, n_features), return_sequences = FALSE,
                     kernel_regularizer = regularizer_l2(0.001)) %>%
          layer_dropout(rate = 0.2) %>%
          layer_dense(units = 1, activation = 'sigmoid')

     model %>% compile(
          optimizer = optimizer_adam(learning_rate = 0.001),
          loss = 'binary_crossentropy',
          metrics = 'accuracy'
     )

     # Step 7: Train the model
     history <- model %>% fit(
          X_train,
          y_train_array,
          epochs = 100,
          batch_size = 512,
          validation_split = 0.2
     )

     # Step 8: Evaluate the model
     score <- model %>% evaluate(X_test, y_test_array)
     cat('Test loss:', score$loss, '\n')
     cat('Test accuracy:', score$acc, '\n')

     # Step 9: Make predictions on all data
     d$pred <- model %>% predict(X_reshaped)
     d_all$pred <- model %>% predict(X_all_reshaped)


     # Step 10: Ensure all year-week combinations are present, then smooth predictions by country
     message("Smoothing time series predictions...")
     d_all <- d_all %>%
          group_by(iso_code) %>%
          # Create a complete dataset for each country with all year-week combinations
          complete(year, week, fill = list(pred = NA)) %>%
          mutate(
               # Create ISO week string
               iso_week = paste0(year, "-W", sprintf("%02d", week)),  # ISO week format (e.g., "2021-W01")

               # Directly calculate date_start (Monday) and date_stop (Sunday) for each week
               date_start = ISOweek::ISOweek2date(paste(iso_week, "-1", sep = "")),  # Monday of the week
               date_stop = ISOweek::ISOweek2date(paste(iso_week, "-7", sep = ""))    # Sunday of the week
          ) %>%
          arrange(iso_code, date_start) %>%
          # Remove rows where pred is NA before applying LOESS to avoid errors
          filter(!is.na(pred)) %>%
          mutate(
               # Use LOESS smoothing after ensuring that all rows are present
               pred_smooth = stats::predict(loess(pred ~ as.numeric(date_start), span = 0.01))
          ) %>%
          ungroup()


     if (53 %in% d_all$week) stop("week index is out of bounds")

     #tmp <- d_all[d_all$iso_code == "MOZ",]
     #plot(tmp$date_start, tmp$pred, type='l')
     #lines(tmp$date_start, tmp$pred_smooth, col='red')

     # Save predictions to CSV
     path <- file.path(PATHS$MODEL_INPUT, "pred_psi_suitability.csv")
     write.csv(d_all[,c('country', 'iso_code', 'week', 'date_start', 'date_stop', 'pred', 'pred_smooth')], path, row.names = FALSE)
     message("Predictions saved to: ", path)

     # Save observed data
     path <- file.path(PATHS$MODEL_INPUT, "data_psi_suitability.csv")
     write.csv(d_all, path, row.names = FALSE)
     message("Predictions and all covariates saved to: ", path)

}
