#' Estimate Environmental Suitability (psi) for Cholera Transmission
#'
#' This function loads climate data, ENSO data, and weekly cholera cases data to estimate the environmental
#' suitability for cholera transmission based on various environmental factors. It scales the climate
#' covariates and then fits an LSTM-based RNN model to predict cholera outbreaks. The model is trained
#' using past cholera case data and climate conditions, and predictions are made for environmental
#' suitability (psi) based on the climate covariates.
#'
#' @param PATHS A list containing paths where the data is stored. Typically generated by the `get_paths()` function and should include:
#' \itemize{
#'   \item \strong{DATA_CHOLERA_WEEKLY}: Path to the directory containing processed combined weekly cholera cases data (WHO+JHU+SUPP sources).
#'   \item \strong{DATA_ELEVATION}: Path to the directory where elevation data is stored.
#'   \item \strong{MODEL_INPUT}: Path to save the processed model inputs and outputs.
#'   \item \strong{DOCS_FIGURES}: Path to save the generated plots.
#' }
#' @param fit_date_start Date string or NULL. Start date for model fitting period. If NULL, auto-detects from first cholera case data.
#' @param fit_date_stop Date string or NULL. End date for model fitting period. If NULL, auto-detects from last date with both cholera cases and complete ENSO data.  
#' @param pred_date_start Date string or NULL. Start date for prediction period. If NULL, uses fit_date_start.
#' @param pred_date_stop Date string or NULL. End date for prediction period. If NULL, auto-detects from last date with complete ENSO data.
#'
#' @return This function processes climate and cholera case data, fits an LSTM model, makes predictions on
#'         environmental suitability (psi), and saves both the predictions and covariate data. It also
#'         generates a plot showing the model fit (accuracy and loss over training epochs) and saves it to a
#'         specified directory.
#'
#' @details
#' The function performs the following steps:
#' \itemize{
#'   \item Loads processed climate and cholera data.
#'   \item Determines appropriate date ranges for fitting and prediction based on data availability.
#'   \item Validates data completeness within specified date ranges.
#'   \item Scales the climate covariates using training data statistics.
#'   \item Splits the training data into training and validation sets.
#'   \item Builds an LSTM-based recurrent neural network (RNN) model for predicting cholera outbreaks.
#'   \item Trains the model on the training set and evaluates performance on the validation set.
#'   \item Makes predictions on the full prediction period dataset.
#'   \item Applies temporal smoothing to predictions and saves results to specified directories.
#' }
#'
#' @importFrom arrow read_parquet
#' @importFrom utils read.csv
#' @importFrom glue glue
#' @importFrom dplyr left_join select mutate filter group_by summarize ungroup
#' @importFrom tidyr pivot_wider complete
#' @importFrom ISOweek ISOweek2date
#' @importFrom keras3 keras_model_sequential layer_lstm layer_dropout layer_dense compile fit evaluate regularizer_l2 optimizer_adam callback_early_stopping learning_rate_schedule_exponential_decay
#' @importFrom ggplot2 ggplot geom_rect geom_bar geom_line facet_wrap labs scale_y_continuous scale_x_date theme_minimal theme element_text element_line element_blank annotate
#' @importFrom patchwork plot_layout
#' @importFrom cowplot plot_grid
#' @importFrom zoo rollmean
#' @importFrom lubridate today
#' @importFrom magrittr %>%
#' @export
#'
#' @examples
#' \dontrun{
#' # Basic usage with auto-detected date ranges
#' est_suitability(PATHS)
#' 
#' # Historical validation: fit on 2010-2020, predict on 2021-2023  
#' est_suitability(PATHS, 
#'                 fit_date_start = "2010-01-01", 
#'                 fit_date_stop = "2020-12-31",
#'                 pred_date_start = "2021-01-01", 
#'                 pred_date_stop = "2023-12-31")
#' 
#' }
#'
#' @note
#' The LSTM model uses climate variables to predict cholera suitability. The model's predictions are saved as
#' a CSV file and a plot showing the model fit (accuracy and loss) is generated.
#'
#' @seealso
#' \code{\link[keras]{layer_lstm}}, \code{\link[keras]{fit}}, \code{\link[ggplot2]{ggplot}}
#'

est_suitability <- function(PATHS, 
                           # Fitting period (training data)
                           fit_date_start = NULL, 
                           fit_date_stop = NULL,
                           # Prediction period (inference)
                           pred_date_start = NULL, 
                           pred_date_stop = NULL) {

     require(keras3)
     require(tidyr)
     require(ggplot2)
     require(dplyr)

     message("Loading merged suitability data...")
     path <- file.path(PATHS$DATA_CHOLERA_WEEKLY, 'cholera_country_weekly_suitability_data.csv')
     d_all <- read.csv(path, stringsAsFactors = FALSE)
     d_all$date_start <- as.Date(d_all$date_start)
     d_all$date_stop <- as.Date(d_all$date_stop)

     path <- file.path(PATHS$DATA_ELEVATION, 'country_elevation_mean.csv')
     elevation_data <- read.csv(path, stringsAsFactors = FALSE)
     d_all <- merge(d_all, elevation_data[c("iso_code", "elevation")], by="iso_code")

     if (53 %in% d_all$week) stop("week index is out of bounds")

     message("Determining date ranges for fitting and prediction...")
     
     # ============================================================================
     # Date range auto-detection and validation
     # ============================================================================
     
     # Auto-detect fitting date range
     if (is.null(fit_date_start)) {
          # Find first date with actual cholera case data
          fit_date_start <- min(d_all$date_start[!is.na(d_all$cases_binary)], na.rm = TRUE)
          message(glue::glue("Auto-detected cholera data start: {fit_date_start}"))
     } else {
          fit_date_start <- as.Date(fit_date_start)
          message(glue::glue("Using specified fit_date_start: {fit_date_start}"))
     }
     
     if (is.null(fit_date_stop)) {
          # Find last date with both cholera cases AND complete ENSO data for fitting
          enso_cols <- c("DMI", "ENSO3", "ENSO34", "ENSO4")
          complete_cases <- d_all[!is.na(d_all$cases_binary), ]
          enso_complete <- complete_cases[complete.cases(complete_cases[enso_cols]), ]
          if (nrow(enso_complete) > 0) {
               fit_date_stop <- max(enso_complete$date_stop, na.rm = TRUE)
               message(glue::glue("Auto-detected fitting end (cholera + ENSO): {fit_date_stop}"))
          } else {
               stop("No periods found with both cholera case data and complete ENSO data")
          }
     } else {
          fit_date_stop <- as.Date(fit_date_stop)
          message(glue::glue("Using specified fit_date_stop: {fit_date_stop}"))
     }
     
     # Auto-detect prediction date range
     if (is.null(pred_date_start)) {
          pred_date_start <- fit_date_start
          message(glue::glue("Using fit_date_start for prediction start: {pred_date_start}"))
     } else {
          pred_date_start <- as.Date(pred_date_start)
          message(glue::glue("Using specified pred_date_start: {pred_date_start}"))
     }
     
     if (is.null(pred_date_stop)) {
          # Extend to full ENSO data availability for prediction
          enso_cols <- c("DMI", "ENSO3", "ENSO34", "ENSO4")
          enso_complete <- d_all[complete.cases(d_all[enso_cols]), ]
          if (nrow(enso_complete) > 0) {
               pred_date_stop <- max(enso_complete$date_stop, na.rm = TRUE)
               message(glue::glue("Auto-detected prediction end (ENSO availability): {pred_date_stop}"))
          } else {
               stop("No periods found with complete ENSO data for prediction")
          }
     } else {
          pred_date_stop <- as.Date(pred_date_stop)
          message(glue::glue("Using specified pred_date_stop: {pred_date_stop}"))
     }
     
     # Validate date ranges
     if (fit_date_start >= fit_date_stop) stop("fit_date_start must be before fit_date_stop")
     if (pred_date_start >= pred_date_stop) stop("pred_date_start must be before pred_date_stop")
     
     message(glue::glue("Final date ranges:"))
     message(glue::glue("  Fitting: {fit_date_start} to {fit_date_stop} ({as.numeric(fit_date_stop - fit_date_start)} days)"))
     message(glue::glue("  Prediction: {pred_date_start} to {pred_date_stop} ({as.numeric(pred_date_stop - pred_date_start)} days)"))

     message("Adding covariates...")
     covariates <- c(
          "temperature_2m_mean", "temperature_2m_max", "temperature_2m_min",
          "wind_speed_10m_mean", "wind_speed_10m_max", "cloud_cover_mean",
          "shortwave_radiation_sum", "relative_humidity_2m_mean",
          "relative_humidity_2m_max", "relative_humidity_2m_min",
          "dew_point_2m_mean", "dew_point_2m_min", "dew_point_2m_max",
          "precipitation_sum", "snowfall_sum", "pressure_msl_mean",
          "soil_moisture_0_to_10cm_mean", "et0_fao_evapotranspiration_sum",
          "DMI", "ENSO3", "ENSO34", "ENSO4", "elevation"#,
          #"year", "month", "week"
     )

     # ============================================================================
     # Data completeness validation
     # ============================================================================
     
     # Create separate datasets for fitting and prediction
     message("Creating fitting and prediction datasets...")
     d_fit <- d_all[d_all$date_start >= fit_date_start & d_all$date_stop <= fit_date_stop, ]
     d_pred <- d_all[d_all$date_start >= pred_date_start & d_all$date_stop <= pred_date_stop, ]
     
     message(glue::glue("Dataset sizes: fitting={nrow(d_fit)}, prediction={nrow(d_pred)}"))
     
     # Validate fitting dataset completeness
     message("Validating predictor completeness in fitting period...")
     
     fit_covariate_completeness <- sapply(covariates, function(var) {
          if (var %in% colnames(d_fit)) {
               sum(!is.na(d_fit[[var]])) / nrow(d_fit)
          } else {
               0
          }
     })
     
     incomplete_vars <- names(fit_covariate_completeness)[fit_covariate_completeness < 0.95]
     if (length(incomplete_vars) > 0) {
          warning(glue::glue("Covariates with <95% completeness in fitting period: {paste(incomplete_vars, collapse=', ')}"))
          for (var in incomplete_vars) {
               pct <- round(fit_covariate_completeness[var] * 100, 1)
               message(glue::glue("  {var}: {pct}% complete"))
          }
     }
     
     # Check for countries with poor covariate completeness in fitting period
     if (nrow(d_fit) > 0) {
          fit_country_completeness <- d_fit %>%
               dplyr::group_by(iso_code) %>%
               dplyr::summarise(
                    n_obs = dplyr::n(),
                    n_complete = sum(complete.cases(dplyr::select(dplyr::cur_data(), dplyr::all_of(covariates)))),
                    completeness = n_complete / n_obs,
                    .groups = 'drop'
               ) %>%
               dplyr::filter(completeness < 0.90)
          
          if (nrow(fit_country_completeness) > 0) {
               warning(glue::glue("Countries with <90% covariate completeness in fitting period: {paste(fit_country_completeness$iso_code, collapse=', ')}"))
          }
     }
     
     # Overall fitting completeness check
     fit_overall_completeness <- sum(complete.cases(d_fit[covariates])) / nrow(d_fit)
     message(glue::glue("Fitting period covariate completeness: {round(fit_overall_completeness * 100, 1)}%"))
     
     if (fit_overall_completeness < 0.80) {
          stop(glue::glue("Insufficient data completeness ({round(fit_overall_completeness*100,1)}%) in fitting period {fit_date_start} to {fit_date_stop}. Consider adjusting date ranges or addressing missing data."))
     }
     
     # Check prediction period completeness (warning only)
     message("Validating predictor completeness in prediction period...")
     pred_overall_completeness <- sum(complete.cases(d_pred[covariates])) / nrow(d_pred)
     message(glue::glue("Prediction period covariate completeness: {round(pred_overall_completeness * 100, 1)}%"))
     
     if (pred_overall_completeness < 0.80) {
          warning(glue::glue("Low data completeness ({round(pred_overall_completeness*100,1)}%) in prediction period {pred_date_start} to {pred_date_stop}. Predictions may be less reliable."))
     }

     # ============================================================================
     # Prepare fitting and prediction datasets
     # ============================================================================
     
     message("Preparing datasets with complete cases...")
     
     # Process fitting dataset (for model training)
     X_fit_all <- d_fit[, colnames(d_fit) %in% covariates]
     sel_fit <- complete.cases(X_fit_all)
     d_fit <- d_fit[sel_fit, ]
     X_fit_all <- X_fit_all[sel_fit, ]
     
     # Extract training data (only cases with known binary outcomes)
     d_train <- d_fit[!is.na(d_fit$cases_binary), ]
     y_train <- d_train$cases_binary
     X_train <- d_train[, colnames(d_train) %in% covariates]
     
     sel_train <- complete.cases(X_train)
     d_train <- d_train[sel_train, ]
     y_train <- y_train[sel_train]
     X_train <- X_train[sel_train, ]
     
     # Process prediction dataset (full period)
     X_pred_all <- d_pred[, colnames(d_pred) %in% covariates]
     sel_pred <- complete.cases(X_pred_all)
     d_pred <- d_pred[sel_pred, ]
     X_pred_all <- X_pred_all[sel_pred, ]
     
     message(glue::glue("Training samples: {nrow(X_train)} (from fitting period)"))
     message(glue::glue("Prediction samples: {nrow(X_pred_all)} (full prediction period)"))
     
     if (nrow(X_train) < 100) {
          stop(glue::glue("Insufficient training data ({nrow(X_train)} samples). Consider expanding fit_date_start/fit_date_stop or addressing missing data."))
     }

     # ============================================================================
     # Feature scaling
     # ============================================================================
     
     message("Standardizing features...")
     
     # Fit scaler on training data only
     X_train_scaled <- scale(X_train)
     
     # Apply same scaling to prediction data
     scaler_center <- attr(X_train_scaled, "scaled:center")
     scaler_scale <- attr(X_train_scaled, "scaled:scale")
     X_pred_all_scaled <- scale(X_pred_all, center = scaler_center, scale = scaler_scale)

     # ============================================================================
     # Reshape data for LSTM and create train/test split
     # ============================================================================
     
     timesteps <- 2  # Using 2 time steps for LSTM
     n_features <- ncol(X_train_scaled)
     
     message(glue::glue("Reshaping data: {n_features} features, {timesteps} timesteps"))
     
     # Create 3D arrays for LSTM: (samples, timesteps, features)
     X_train_reshaped <- array(X_train_scaled, dim = c(nrow(X_train_scaled), timesteps, n_features))
     X_pred_all_reshaped <- array(X_pred_all_scaled, dim = c(nrow(X_pred_all_scaled), timesteps, n_features))
     
     # Split training data into train/validation
     set.seed(99)
     train_indices <- sample(1:nrow(X_train_reshaped), 0.8 * nrow(X_train_reshaped))
     
     X_train_model <- X_train_reshaped[train_indices, , ]
     y_train_model <- y_train[train_indices]
     X_val_model <- X_train_reshaped[-train_indices, , ]
     y_val_model <- y_train[-train_indices]

     # Reshape target to 2D: (samples, 1)
     y_train_model_array <- array(y_train_model, dim = c(length(y_train_model), 1))
     y_val_model_array <- array(y_val_model, dim = c(length(y_val_model), 1))

     # Display dataset statistics
     message(glue::glue('Training samples: {dim(X_train_model)[1]} ({sum(y_train_model == 1)} positive)'))
     message(glue::glue('Validation samples: {dim(X_val_model)[1]} ({sum(y_val_model == 1)} positive)'))
     message(glue::glue('Total fitting samples: {nrow(X_train_reshaped)} (from {fit_date_start} to {fit_date_stop})'))
     message(glue::glue('Prediction samples: {nrow(X_pred_all_reshaped)} (from {pred_date_start} to {pred_date_stop})'))



     message("Compiling LSTM model...")
     # Define an exponential decay schedule for learning rate using keras3
     lr_schedule <- keras3::learning_rate_schedule_exponential_decay(
          initial_learning_rate = 0.001,
          decay_steps = 10000,
          decay_rate = 0.9
     )

     # Step 6: Build and Compile the LSTM model
     model <- keras_model_sequential()
     model <- layer_lstm(model, units = 500, input_shape = c(timesteps, n_features), return_sequences = TRUE,
                     kernel_regularizer = regularizer_l2(0.001))
     model <- layer_dropout(model, rate = 0.5)
     model <- layer_lstm(model, units = 250, return_sequences = TRUE, kernel_regularizer = regularizer_l2(0.001))
     model <- layer_dropout(model, rate = 0.5)
     model <- layer_lstm(model, units = 100, return_sequences = FALSE, kernel_regularizer = regularizer_l2(0.001))
     model <- layer_dropout(model, rate = 0.5)
     model <- layer_dense(model, units = 1, activation = 'sigmoid')

     # Compile the model with the learning rate schedule
     compile(model,
          optimizer = optimizer_adam(learning_rate = lr_schedule),  # Using the exponential decay schedule
          loss = 'binary_crossentropy',
          metrics = 'accuracy'
     )

     print(model)

     message("Training LSTM model...")
     # Train the model using the training subset
     history <- fit(model,
          X_train_model,
          y_train_model_array,
          epochs = 200,
          batch_size = 1024,
          validation_data = list(X_val_model, y_val_model_array),  # Use explicit validation data
          callbacks = list(callback_early_stopping(patience = 10))  # Early stopping for stability
     )

     # Evaluate the model on validation set
     message("Calculating model performance...")
     score <- evaluate(model, X_val_model, y_val_model_array)
     message(glue::glue('Validation loss: {round(score$loss, 4)}'))
     message(glue::glue('Validation accuracy: {round(score$acc, 4)}'))




     df <- data.frame(
          epoch = 1:length(history$metrics$loss),
          loss = history$metrics$loss,
          val_loss = history$metrics$val_loss,
          accuracy = history$metrics$accuracy,
          val_accuracy = history$metrics$val_accuracy
     )

     # Get final model performance metrics
     final_loss <- score$loss
     final_accuracy <- score$acc



     df <- data.frame(
          epoch = 1:length(history$metrics$loss),
          loss = history$metrics$loss,
          val_loss = history$metrics$val_loss,
          accuracy = history$metrics$accuracy,
          val_accuracy = history$metrics$val_accuracy
     )

     loss_plot <-
          ggplot(df, aes(x = epoch)) +
          geom_line(aes(y = loss, color = "Training Loss")) +
          geom_line(aes(y = val_loss, color = "Validation Loss")) +
          labs(x = "Epoch", y = "Loss", title = "Training and Validation Loss") +
          scale_color_manual(name = "",
                             values = c("Training Loss" = "blue3",
                                        "Validation Loss" = "red3")) +
          theme_bw() +  # White background
          theme(legend.position = "bottom") +
          annotate("text", x = max(df$epoch), y = max(df$loss),
                   label = paste("Final Test Loss:", round(final_loss, 2)),
                   vjust=5, hjust=1, color = "blue3")

     accuracy_plot <-
          ggplot(df, aes(x = epoch)) +
          geom_line(aes(y = accuracy, color = "Training Accuracy")) +
          geom_line(aes(y = val_accuracy, color = "Validation Accuracy")) +
          labs(x = "Epoch", y = "Accuracy", title = "Training and Validation Accuracy") +
          scale_color_manual(name = "",
                             values = c("Training Accuracy" = "green4",
                                        "Validation Accuracy" = "darkorange")) +
          theme_bw() +  # White background
          theme(legend.position = "bottom") +
          annotate("text", x = max(df$epoch), y = min(df$accuracy),
                   label = paste("Final Test Accuracy:", round(final_accuracy, 2)),
                   vjust=-5,     hjust=1, color = "green4")

     combined_plot <- cowplot::plot_grid(accuracy_plot, loss_plot, labels = "AUTO", ncol = 2, align = "v")

     print(combined_plot)

     plot_file <- file.path(PATHS$DOCS_FIGURES, "suitability_LSTM_fit.png")
     ggplot2::ggsave(filename = plot_file, plot = combined_plot, width = 8, height = 4, units = "in", dpi = 300)
     message(glue::glue("Model fit plot saved to: {plot_file}"))




     message("Generating predictions...")
     
     # ============================================================================
     # Generate predictions for all periods
     # ============================================================================
     
     # Predict on training data (for model validation)
     d_train$pred <- predict(model, X_train_reshaped)
     
     # Predict on full prediction dataset
     d_pred$pred <- predict(model, X_pred_all_reshaped)


     # ============================================================================
     # Temporal smoothing of predictions
     # ============================================================================
     
     message("Smoothing time series predictions on weekly time scale...")
     d_pred_smooth <- d_pred %>%
          dplyr::group_by(iso_code) %>%
          # Create a complete dataset for each country with all year-week combinations
          tidyr::complete(year, week, fill = list(pred = NA)) %>%
          dplyr::mutate(
               # Create ISO week string
               iso_week = paste0(year, "-W", sprintf("%02d", week)),  # ISO week format (e.g., "2021-W01")

               # Directly calculate date_start (Monday) and date_stop (Sunday) for each week
               date_start = ISOweek::ISOweek2date(paste(iso_week, "-1", sep = "")),  # Monday of the week
               date_stop = ISOweek::ISOweek2date(paste(iso_week, "-7", sep = ""))    # Sunday of the week
          ) %>%
          dplyr::arrange(iso_code, date_start) %>%
          # Remove rows where pred is NA before applying LOESS to avoid errors
          dplyr::filter(!is.na(pred)) %>%
          dplyr::mutate(
               pred_smooth = inv_logit(stats::predict(loess(logit(pred) ~ as.numeric(date_start), span = 0.01)))
          ) %>%
          dplyr::ungroup()


     if (53 %in% d_pred_smooth$week) stop("week index is out of bounds")

     message("Smoothing time series predictions on daily time scale...")

     d_pred_weekly <- as.data.frame(d_pred_smooth[,c('iso_code', 'week', 'date_start', 'date_stop', 'pred', 'pred_smooth')])

     split_data <- split(d_pred_weekly, d_pred_weekly$iso_code)

     d_pred_daily <- expand.grid(list(date = seq(min(as.Date(d_pred_weekly$date_start)),
                                                 max(as.Date(d_pred_weekly$date_stop)),
                                                 by='day'),
                                      iso_code = unique(d_pred_weekly$iso_code)))

     d_pred_daily$week <- lubridate::isoweek(d_pred_daily$date)
     d_pred_daily <- merge(d_pred_daily, d_pred_weekly, by=c('week', 'iso_code'))

     split_data <- split(d_pred_weekly, d_pred_weekly$iso_code)

     smoothed_list <-
          lapply(split_data, function(x) {

               iso_code <- unique(x$iso_code)
               date_min <- min(as.Date(x$date_start))
               date_max <- max(as.Date(x$date_stop))
               x <- x[,c('date_start', 'pred')]

               d_pred_daily <- data.frame(date = seq(date_min, date_max, by='day'))
               d_pred_daily <- merge(d_pred_daily, x, by.x='date', by.y='date_start', all.x=TRUE)
               d_pred_daily$pred <- zoo::na.locf(d_pred_daily$pred)
               d_pred_daily$pred_smooth <- inv_logit(stats::predict(loess(logit(pred) ~ as.numeric(date), span = 0.005, data=d_pred_daily)))

               #sel <- 1:1000
               #plot(d_pred_daily$date[sel], d_pred_daily$pred[sel], type='l')
               #lines(d_pred_daily$date[sel], d_pred_daily$pred_smooth[sel], col='red')

               data.frame(
                    iso_code = iso_code,
                    d_pred_daily
               )

          })

     d_pred_daily <- do.call(rbind, smoothed_list)
     row.names(d_pred_daily) <- NULL
     rm(split_data)
     rm(smoothed_list)



     # ELSE method == 'poisson' {

     # Fit LSTM model with count-based link function directly to the cholera case data

     # psi_jt = P(x > k)

     # }


     # Save predictions to CSV
     path <- file.path(PATHS$MODEL_INPUT, "pred_psi_suitability_week.csv")
     write.csv(d_pred_weekly, path, row.names = FALSE)
     message("Predictions saved to: ", path)

     path <- file.path(PATHS$MODEL_INPUT, "pred_psi_suitability_day.csv")
     write.csv(d_pred_daily, path, row.names = FALSE)
     message("Predictions saved to: ", path)

     # Save observed data and metadata
     d_all_output <- rbind(
          cbind(d_train[, !colnames(d_train) %in% c("pred")], data_type = "training"),
          cbind(d_pred[, !colnames(d_pred) %in% c("pred")], data_type = "prediction")
     )
     path <- file.path(PATHS$MODEL_INPUT, "data_psi_suitability.csv")
     write.csv(d_all_output, path, row.names = FALSE)
     message("All covariates and metadata saved to: ", path)
     
     # Save model configuration and date ranges
     config_info <- list(
          fit_date_start = as.character(fit_date_start),
          fit_date_stop = as.character(fit_date_stop),
          pred_date_start = as.character(pred_date_start),
          pred_date_stop = as.character(pred_date_stop),
          n_training_samples = nrow(d_train),
          n_prediction_samples = nrow(d_pred),
          model_performance = list(
               validation_loss = final_loss,
               validation_accuracy = final_accuracy
          ),
          covariates = covariates
     )
     
     config_path <- file.path(PATHS$MODEL_INPUT, "psi_suitability_config.json")
     jsonlite::write_json(config_info, config_path, pretty = TRUE, auto_unbox = TRUE)
     message("Model configuration saved to: ", config_path)

}
