#' Estimate Environmental Suitability (psi) for Cholera Transmission
#'
#' This function loads climate data, ENSO data, and weekly cholera cases data to estimate the environmental
#' suitability for cholera transmission based on various environmental factors. It uses lagged climate
#' covariates, scales them, and then fits an LSTM-based RNN model to predict cholera outbreaks. The model
#' is trained using past cholera case data and climate conditions, and predictions are made for environmental
#' suitability (psi) based on the climate covariates.
#'
#' @param PATHS A list containing paths where the data is stored. Typically generated by the `get_paths()` function and should include:
#' \itemize{
#'   \item \strong{DATA_CHOLERA_WEEKLY}: Path to the directory containing processed combined weekly cholera cases data (WHO+JHU+SUPP sources).
#'   \item \strong{DATA_ELEVATION}: Path to the directory where elevation data is stored.
#'   \item \strong{MODEL_INPUT}: Path to save the processed model inputs and outputs.
#'   \item \strong{DOCS_FIGURES}: Path to save the generated plots.
#' }
#'
#' @return This function processes climate and cholera case data, fits an LSTM model, makes predictions on
#'         environmental suitability (psi), and saves both the predictions and covariate data. It also
#'         generates a plot showing the model fit (accuracy and loss over training epochs) and saves it to a
#'         specified directory.
#'
#' @details
#' The function performs the following steps:
#' \itemize{
#'   \item Loads processed climate and cholera data.
#'   \item Creates lagged versions of relevant climate variables (e.g., temperature, precipitation, ENSO data).
#'   \item Scales the climate covariates.
#'   \item Splits the data into training and test sets.
#'   \item Builds an LSTM-based recurrent neural network (RNN) model for predicting cholera outbreaks.
#'   \item Trains the model on the training set and evaluates its performance on the test set.
#'   \item Makes predictions on both the test set and the full dataset.
#'   \item Saves the predictions, covariate data, and model fit plots to the specified directories.
#' }
#'
#' @importFrom arrow read_parquet
#' @importFrom utils read.csv
#' @importFrom glue glue
#' @importFrom dplyr left_join select mutate filter group_by summarize ungroup
#' @importFrom tidyr pivot_wider complete
#' @importFrom ISOweek ISOweek2date
#' @importFrom keras3 keras_model_sequential layer_lstm layer_dropout layer_dense compile fit evaluate regularizer_l2 optimizer_adam callback_early_stopping learning_rate_schedule_exponential_decay
#' @importFrom ggplot2 ggplot geom_rect geom_bar geom_line facet_wrap labs scale_y_continuous scale_x_date theme_minimal theme element_text element_line element_blank annotate
#' @importFrom patchwork plot_layout
#' @importFrom cowplot plot_grid
#' @importFrom zoo rollmean
#' @importFrom lubridate today
#' @importFrom magrittr %>%
#' @export
#'
#' @examples
#' \dontrun{
#' # Assuming PATHS is defined and points to the correct data directories:
#' est_suitability(PATHS)
#' }
#'
#' @note
#' The LSTM model uses climate variables to predict cholera suitability. The model's predictions are saved as
#' a CSV file and a plot showing the model fit (accuracy and loss) is generated.
#'
#' @seealso
#' \code{\link[keras]{layer_lstm}}, \code{\link[keras]{fit}}, \code{\link[ggplot2]{ggplot}}
#'

est_suitability <- function(PATHS, include_lagged_covariates=FALSE) {

     require(keras3)
     require(tidyr)
     require(ggplot2)
     require(dplyr)

     message("Loading merged suitability data...")
     path <- file.path(PATHS$DATA_CHOLERA_WEEKLY, 'cholera_country_weekly_suitability_data.csv')
     d_all <- read.csv(path, stringsAsFactors = FALSE)
     d_all$date_start <- as.Date(d_all$date_start)
     d_all$date_stop <- as.Date(d_all$date_stop)

     path <- file.path(PATHS$DATA_ELEVATION, 'country_elevation_mean.csv')
     elevation_data <- read.csv(path, stringsAsFactors = FALSE)
     d_all <- merge(d_all, elevation_data[c("iso_code", "elevation")], by="iso_code")

     if (53 %in% d_all$week) stop("week index is out of bounds")

     message("Adding covariates...")
     covariates <- c(
          "temperature_2m_mean", "temperature_2m_max", "temperature_2m_min",
          "wind_speed_10m_mean", "wind_speed_10m_max", "cloud_cover_mean",
          "shortwave_radiation_sum", "relative_humidity_2m_mean",
          "relative_humidity_2m_max", "relative_humidity_2m_min",
          "dew_point_2m_mean", "dew_point_2m_min", "dew_point_2m_max",
          "precipitation_sum", "snowfall_sum", "pressure_msl_mean",
          "soil_moisture_0_to_10cm_mean", "et0_fao_evapotranspiration_sum",
          "DMI", "ENSO3", "ENSO34", "ENSO4", "elevation"#,
          #"year", "month", "week"
     )

     X_all <- d_all[, colnames(d_all) %in% covariates]
     sel <- complete.cases(X_all)
     d_all <- d_all[sel,]
     X_all <- X_all[sel,]

     d <- d_all[!is.na(d_all$cases_binary),]
     y <- d$cases_binary
     X <- d[,colnames(d_all) %in% covariates]

     sel <- complete.cases(X)
     d <- d[sel,]
     y <- y[sel]
     X <- X[sel,]

     # Step 3: Standardize the features (covariates)
     if (include_lagged_covariates) {

          X_all_lagged <- MOSAIC::make_lagged_data(X_all, lags=1:7)
          X_lagged <- MOSAIC::make_lagged_data(X, lags=1:7)

          X_all_scaled <- scale(X_all_lagged)
          X_scaled <- scale(X_lagged)


     } else {

          X_all_scaled <- scale(X_all)
          X_scaled <- scale(X)

     }

     # Step 4: Reshape the data for RNN
     timesteps <- 2  # If using 2 time steps
     n_features <- ncol(X_scaled)  # Number of features

     # Create a 3D array for LSTM: (samples, timesteps, features)
     X_all_reshaped <- array(X_all_scaled, dim = c(nrow(X_all_scaled), timesteps, n_features))
     X_reshaped <- array(X_scaled, dim = c(nrow(X_scaled), timesteps, n_features))

     # Step 5: Split the data into training and testing sets
     set.seed(99)
     train_indices <- sample(1:nrow(X_reshaped), 0.8 * nrow(X_reshaped))
     X_train <- X_reshaped[train_indices, , ]
     y_train <- y[train_indices]
     X_test <- X_reshaped[-train_indices, , ]
     y_test <- y[-train_indices]

     # Reshape target to 2D: (samples, 1)
     y_train_array <- array(y_train, dim = c(length(y_train), 1))
     y_test_array <- array(y_test, dim = c(length(y_test), 1))

     # Display total number of samples and number of positive samples in training and test sets
     cat('Number of samples in training set:', dim(X_train)[1], '\n')
     cat('Number of positive samples in training set:', sum(y_train == 1), '\n')
     cat('Number of samples in test set:', dim(X_test)[1], '\n')
     cat('Number of positive samples in test set:', sum(y_test == 1), '\n')



     message("Compiling LSTM model...")
     # Define an exponential decay schedule for learning rate using keras3
     lr_schedule <- keras3::learning_rate_schedule_exponential_decay(
          initial_learning_rate = 0.001,
          decay_steps = 10000,
          decay_rate = 0.9
     )

     # Step 6: Build and Compile the LSTM model
     model <- keras_model_sequential()
     model <- layer_lstm(model, units = 500, input_shape = c(timesteps, n_features), return_sequences = TRUE,
                     kernel_regularizer = regularizer_l2(0.001))
     model <- layer_dropout(model, rate = 0.5)
     model <- layer_lstm(model, units = 250, return_sequences = TRUE, kernel_regularizer = regularizer_l2(0.001))
     model <- layer_dropout(model, rate = 0.5)
     model <- layer_lstm(model, units = 100, return_sequences = FALSE, kernel_regularizer = regularizer_l2(0.001))
     model <- layer_dropout(model, rate = 0.5)
     model <- layer_dense(model, units = 1, activation = 'sigmoid')

     # Compile the model with the learning rate schedule
     compile(model,
          optimizer = optimizer_adam(learning_rate = lr_schedule),  # Using the exponential decay schedule
          loss = 'binary_crossentropy',
          metrics = 'accuracy'
     )

     print(model)

     message("Training LSTM model...")
     # Step 7: Train the model
     history <- fit(model,
          X_train,
          y_train_array,
          epochs = 200,
          batch_size = 1024,
          validation_split = 0.2,
          callbacks = list(callback_early_stopping(patience = 10))  # Early stopping for stability
     )

     # Step 8: Evaluate the model
     message("Calculating overall model fit...")
     score <- evaluate(model, X_test, y_test_array)
     cat('Test loss:', score$loss, '\n')
     cat('Test accuracy:', score$acc, '\n')




     df <- data.frame(
          epoch = 1:length(history$metrics$loss),
          loss = history$metrics$loss,
          val_loss = history$metrics$val_loss,
          accuracy = history$metrics$accuracy,
          val_accuracy = history$metrics$val_accuracy
     )

     # Step 8: Evaluate the model and get final test loss and accuracy
     message("Calculating overall model fit...")
     score <- evaluate(model, X_test, y_test_array)
     final_loss <- score$loss
     final_accuracy <- score$acc

     cat('Test loss:', final_loss, '\n')
     cat('Test accuracy:', final_accuracy, '\n')



     df <- data.frame(
          epoch = 1:length(history$metrics$loss),
          loss = history$metrics$loss,
          val_loss = history$metrics$val_loss,
          accuracy = history$metrics$accuracy,
          val_accuracy = history$metrics$val_accuracy
     )

     loss_plot <-
          ggplot(df, aes(x = epoch)) +
          geom_line(aes(y = loss, color = "Training Loss")) +
          geom_line(aes(y = val_loss, color = "Validation Loss")) +
          labs(x = "Epoch", y = "Loss", title = "Training and Validation Loss") +
          scale_color_manual(name = "",
                             values = c("Training Loss" = "blue3",
                                        "Validation Loss" = "red3")) +
          theme_bw() +  # White background
          theme(legend.position = "bottom") +
          annotate("text", x = max(df$epoch), y = max(df$loss),
                   label = paste("Final Test Loss:", round(final_loss, 2)),
                   vjust=5, hjust=1, color = "blue3")

     accuracy_plot <-
          ggplot(df, aes(x = epoch)) +
          geom_line(aes(y = accuracy, color = "Training Accuracy")) +
          geom_line(aes(y = val_accuracy, color = "Validation Accuracy")) +
          labs(x = "Epoch", y = "Accuracy", title = "Training and Validation Accuracy") +
          scale_color_manual(name = "",
                             values = c("Training Accuracy" = "green4",
                                        "Validation Accuracy" = "darkorange")) +
          theme_bw() +  # White background
          theme(legend.position = "bottom") +
          annotate("text", x = max(df$epoch), y = min(df$accuracy),
                   label = paste("Final Test Accuracy:", round(final_accuracy, 2)),
                   vjust=-5,     hjust=1, color = "green4")

     combined_plot <- cowplot::plot_grid(accuracy_plot, loss_plot, labels = "AUTO", ncol = 2, align = "v")

     print(combined_plot)

     plot_file <- file.path(PATHS$DOCS_FIGURES, "suitability_LSTM_fit.png")
     ggplot2::ggsave(filename = plot_file, plot = combined_plot, width = 8, height = 4, units = "in", dpi = 300)
     message(glue::glue("Model fit plot saved to: {plot_file}"))




     message("Predicting response values...")
     # Step 9: Make predictions on all data
     d$pred <- predict(model, X_reshaped)
     d_all$pred <- predict(model, X_all_reshaped)


     # Step 10: Ensure all year-week combinations are present, then smooth predictions by country
     message("Smoothing time series predictions on weekly time scale...")
     d_all <- d_all %>%
          group_by(iso_code) %>%
          # Create a complete dataset for each country with all year-week combinations
          tidyr::complete(year, week, fill = list(pred = NA)) %>%
          mutate(
               # Create ISO week string
               iso_week = paste0(year, "-W", sprintf("%02d", week)),  # ISO week format (e.g., "2021-W01")

               # Directly calculate date_start (Monday) and date_stop (Sunday) for each week
               date_start = ISOweek::ISOweek2date(paste(iso_week, "-1", sep = "")),  # Monday of the week
               date_stop = ISOweek::ISOweek2date(paste(iso_week, "-7", sep = ""))    # Sunday of the week
          ) %>%
          arrange(iso_code, date_start) %>%
          # Remove rows where pred is NA before applying LOESS to avoid errors
          filter(!is.na(pred)) %>%
          mutate(
               pred_smooth = inv_logit(stats::predict(loess(logit(pred) ~ as.numeric(date_start), span = 0.01)))
          ) %>%
          ungroup()


     if (53 %in% d_all$week) stop("week index is out of bounds")

     message("Smoothing time series predictions on daily time scale...")

     d_pred_weekly <- as.data.frame(d_all[,c('iso_code', 'week', 'date_start', 'date_stop', 'pred', 'pred_smooth')])

     split_data <- split(d_pred_weekly, d_pred_weekly$iso_code)

     d_pred_daily <- expand.grid(list(date = seq(min(as.Date(d_pred_weekly$date_start)),
                                                 max(as.Date(d_pred_weekly$date_stop)),
                                                 by='day'),
                                      iso_code = unique(d_pred_weekly$iso_code)))

     d_pred_daily$week <- lubridate::isoweek(d_pred_daily$date)
     d_pred_daily <- merge(d_pred_daily, d_pred_weekly, by=c('week', 'iso_code'))

     split_data <- split(d_pred_weekly, d_pred_weekly$iso_code)

     smoothed_list <-
          lapply(split_data, function(x) {

               iso_code <- unique(x$iso_code)
               date_min <- min(as.Date(x$date_start))
               date_max <- max(as.Date(x$date_stop))
               x <- x[,c('date_start', 'pred')]

               d_pred_daily <- data.frame(date = seq(date_min, date_max, by='day'))
               d_pred_daily <- merge(d_pred_daily, x, by.x='date', by.y='date_start', all.x=TRUE)
               d_pred_daily$pred <- zoo::na.locf(d_pred_daily$pred)
               d_pred_daily$pred_smooth <- inv_logit(stats::predict(loess(logit(pred) ~ as.numeric(date), span = 0.005, data=d_pred_daily)))

               #sel <- 1:1000
               #plot(d_pred_daily$date[sel], d_pred_daily$pred[sel], type='l')
               #lines(d_pred_daily$date[sel], d_pred_daily$pred_smooth[sel], col='red')

               data.frame(
                    iso_code = iso_code,
                    d_pred_daily
               )

          })

     d_pred_daily <- do.call(rbind, smoothed_list)
     row.names(d_pred_daily) <- NULL
     rm(split_data)
     rm(smoothed_list)



     # ELSE method == 'poisson' {

     # Fit LSTM model with count-based link function directly to the cholera case data

     # psi_jt = P(x > k)

     # }


     # Save predictions to CSV
     path <- file.path(PATHS$MODEL_INPUT, "pred_psi_suitability_week.csv")
     write.csv(d_pred_weekly, path, row.names = FALSE)
     message("Predictions saved to: ", path)

     path <- file.path(PATHS$MODEL_INPUT, "pred_psi_suitability_day.csv")
     write.csv(d_pred_daily, path, row.names = FALSE)
     message("Predictions saved to: ", path)

     # Save observed data
     path <- file.path(PATHS$MODEL_INPUT, "data_psi_suitability.csv")
     write.csv(d_all, path, row.names = FALSE)
     message("Predictions and all covariates saved to: ", path)

}
