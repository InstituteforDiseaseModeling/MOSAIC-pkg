#' Process Environmental Suitability Data for Cholera Transmission
#'
#' This function loads climate data, ENSO data, and weekly cholera cases data, processes them, and merges them into a dataset to estimate the environmental suitability for cholera transmission based on various environmental factors. It also creates a binary environmental suitability indicator based on case patterns.
#'
#' @param PATHS A list containing paths where the data is stored. Typically generated by the `get_paths()` function and should include:
#' \itemize{
#'   \item \strong{DATA_CLIMATE}: Path to the directory where processed climate data is stored.
#'   \item \strong{DATA_ENSO}: Path to the directory where ENSO data is stored.
#'   \item \strong{DATA_CHOLERA_WEEKLY}: Path to the directory containing processed combined weekly cholera cases data (WHO+JHU+SUPP sources).
#' }
#' @param cutoff Numeric threshold for case counts. Used when use_epidemic_peaks = FALSE.
#'   Weeks with cases >= cutoff are considered outbreak periods for the environmental
#'   suitability calculation. Default behavior uses this value to create the binary indicator.
#' @param use_epidemic_peaks Logical. If TRUE, uses epidemic peak periods from
#'   est_epidemic_peaks() to define environmental suitability using exact peak_start
#'   to peak_stop dates. Provides deterministic outbreak detection without any
#'   modifications or lead-up periods. Default FALSE for backwards compatibility.
#' @param date_start Optional start year for data filtering. If NULL, automatically determined
#'   from case data (one year prior to earliest case observation).
#' @param date_stop Optional end year for data filtering. If NULL, automatically determined
#'   from latest ENSO data availability.
#' @param forecast_mode Logical. If TRUE, only includes variables that can support forecasting
#'   at the specified horizon. Excludes case-dependent epidemic memory and spatial import
#'   pressure variables. Default TRUE.
#' @param forecast_horizon Numeric. Forecasting horizon in months. Variables dependent on
#'   surveillance data beyond this horizon are excluded in forecast mode. Default 3 months.
#' @param include_lags Logical. If TRUE, includes time-lagged climate variables with
#'   epidemiologically-informed lag periods specific to each variable. Default FALSE.
#'
#' @return This function processes the data and merges the climate, ENSO, and cholera cases data into a single dataset. It creates a \code{cases_binary} column indicating environmental suitability based on case patterns using sophisticated temporal logic. The processed dataset is saved as a CSV file.
#'
#' @details
#' The function performs the following key steps:
#' \itemize{
#'   \item Loads and merges climate data (temperature, precipitation, etc.) from multiple countries
#'   \item Loads ENSO and DMI climate index data
#'   \item Loads combined weekly cholera case surveillance data from WHO, JHU, and supplemental sources
#'   \item Merges all datasets by country, year, and week
#'   \item Creates environmental suitability indicator (\code{cases_binary}) using either epidemic peak-based or threshold-based methods
#'   \item Saves the complete merged dataset for use in environmental suitability modeling
#' }
#'
#' \strong{Environmental Suitability Methods:}
#' \itemize{
#'   \item \strong{Epidemic Peak-based} (use_epidemic_peaks = TRUE): Uses deterministic outbreak detection from est_epidemic_peaks() using exact peak_start to peak_stop dates without any modifications
#'   \item \strong{Threshold-based} (use_epidemic_peaks = FALSE): Uses simple case count cutoffs with temporal lead-up logic
#' }
#'
#' The epidemic peak-based method provides deterministic, epidemiologically meaningful outbreak periods
#' and is recommended for consistent environmental suitability modeling.
#'
#' @importFrom arrow read_parquet
#' @importFrom utils read.csv write.csv
#' @importFrom glue glue
#' @importFrom dplyr left_join select_if select
#' @importFrom tidyr pivot_wider
#'
#' @seealso
#' \code{\link{get_cases_binary}} for the threshold-based environmental suitability method
#' \code{\link{get_cases_binary_from_peaks}} for the epidemic peak-based method
#' \code{\link{est_epidemic_peaks}} for epidemic peak detection
#'
#' @export
process_suitability_data <- function(PATHS, cutoff, use_epidemic_peaks = FALSE,
                                    date_start = NULL, date_stop = NULL,
                                    forecast_mode = TRUE, forecast_horizon = 3, include_lags = FALSE) {

     requireNamespace('arrow')
     requireNamespace('ISOweek')
     requireNamespace('dplyr')
     requireNamespace('tidyr')
     requireNamespace('zoo')
     requireNamespace('lubridate')
     requireNamespace('rlang')
     requireNamespace('slider')

     # Load climate data for all countries (weekly)
     message("Loading climate data...")

     # List all weekly climate data files
     climate_files <- list.files(PATHS$DATA_CLIMATE, pattern = "weekly\\.parquet$", full.names = TRUE)

     # Initialize an empty list to store climate data for each ISO code
     climate_data_list <- list()

     # Loop through each climate file and load the data
     for (climate_file in climate_files) {
          climate_data <- arrow::read_parquet(climate_file)
          climate_data_list <- append(climate_data_list, list(climate_data))
     }

     # Combine all climate data into one data frame
     combined_climate_data <- do.call(rbind, climate_data_list)
     rm(climate_data_list)

     # Load ENSO data (compiled ENSO and DMI data)
     message("Loading ENSO and DMI data...")
     enso_file <- file.path(PATHS$DATA_ENSO, "compiled_ENSO_1970_2025_weekly.csv")
     enso_data <- utils::read.csv(enso_file, stringsAsFactors = FALSE)

     # Load weekly cholera cases data (combined WHO+JHU+SUPP sources)
     message("Loading combined weekly cholera cases data (WHO+JHU+SUPP)...")
     cases_file <- file.path(PATHS$DATA_CHOLERA_WEEKLY, "cholera_surveillance_weekly_combined.csv")
     cases_data <- utils::read.csv(cases_file, stringsAsFactors = FALSE)

     # Determine date range based on case data and ENSO data (or use provided dates)
     message("Determining optimal date range...")
     
     if (is.null(date_start)) {
          # Find earliest case observation (excluding zeros and NAs)
          cases_with_dates <- cases_data[!is.na(cases_data$cases) & cases_data$cases > 0, ]
          if (nrow(cases_with_dates) > 0) {
               earliest_case_year <- min(cases_with_dates$year, na.rm = TRUE)
               start_year <- earliest_case_year - 1  # Start one year prior
               message(sprintf("  - Earliest case observation: %d", earliest_case_year))
          } else {
               earliest_case_year <- NA
               start_year <- 2000  # Fallback if no case data found
               message("  - No case observations found, using fallback start year")
          }
     } else {
          start_year <- date_start
          message(sprintf("  - Using provided start year: %d", start_year))
     }
     
     if (is.null(date_stop)) {
          # Find the actual end date of raw ENSO data (not interpolated/forecasted)
          end_year <- max(enso_data$year, na.rm = TRUE)
          message(sprintf("  - Latest ENSO data: %d (using max date from raw ENSO dataset)", end_year))
     } else {
          end_year <- date_stop
          message(sprintf("  - Using provided end year: %d", end_year))
     }
     
     message(sprintf("  - Final date range: %d-%d", start_year, end_year))

     # Filter all datasets to this date range and MOSAIC countries only
     message("Filtering datasets to optimal date range and MOSAIC countries...")
     
     # Filter by date range
     combined_climate_data <- combined_climate_data[combined_climate_data$year >= start_year & 
                                                   combined_climate_data$year <= end_year, ]
     enso_data <- enso_data[enso_data$year >= start_year & enso_data$year <= end_year, ]
     cases_data <- cases_data[cases_data$year >= start_year & cases_data$year <= end_year, ]
     
     # Filter by MOSAIC countries only
     combined_climate_data <- combined_climate_data[combined_climate_data$iso_code %in% iso_codes_mosaic, ]
     cases_data <- cases_data[cases_data$iso_code %in% iso_codes_mosaic, ]
     
     message(sprintf("  - Filtered to %d MOSAIC countries: %s", 
                    length(iso_codes_mosaic), 
                    paste(head(sort(iso_codes_mosaic), 8), collapse = ", ")))
     if (length(iso_codes_mosaic) > 8) {
          message(sprintf("    ... and %d more countries", length(iso_codes_mosaic) - 8))
     }

     message("Merging datasets...")
     # Convert the climate data to wide format, with variable names as new columns
     combined_climate_data_wide <- tidyr::pivot_wider(
          combined_climate_data,
          names_from = variable_name,
          values_from = value
     )

     combined_climate_data_wide <- combined_climate_data_wide %>%
          dplyr::select(-date_start, -date_stop)

     # Convert ENSO data to wide format (DMI, ENSO3, ENSO34, ENSO4 as new columns)
     enso_data_wide <- tidyr::pivot_wider(
          enso_data,
          names_from = variable,
          values_from = value
     )

     enso_data_wide <- enso_data_wide %>%
          dplyr::select(-date_start, -date_stop)

     # Merge datasets: Merge cases data with wide climate data and ENSO data
     combined_climate_data_wide <- merge(combined_climate_data_wide, enso_data_wide, by = c("year", "week"), all.x = TRUE)
     d <- merge(cases_data, combined_climate_data_wide, by = c("iso_code", "year", "week"), all.y = TRUE)

     message(sprintf("After merge: %d observations, columns: %s",
                     nrow(d), paste(head(names(d), 10), collapse = ", ")))


     # Limit week values to 1-52
     d <- d %>%
          dplyr::filter(week >= 1 & week <= 52)

     # Create ISO week strings
     d$iso_week <- paste0(d$year, "-W", sprintf("%02d", d$week))

     # Convert ISO week to center date (Thursday - middle of week)
     d$date <- ISOweek::ISOweek2date(paste0(d$iso_week, "-4"))

     # Remove the temporary iso_week column
     d$iso_week <- NULL

     # Ensure month column has integer month values
     d$month <- as.integer(format(d$date, "%m"))

     # Remove columns that are entirely NA or NaN
     d <- dplyr::select_if(d, ~!all(is.na(.) | is.nan(.)))
     
     # Filter to only include time periods with ENSO data (since ENSO is the limiting factor for forecasting)
     d <- d %>% dplyr::filter(!is.na(ENSO34))  # Use ENSO34 as the reference since it's always present
     
     message(sprintf("  - Filtered to ENSO data availability: %d observations retained", nrow(d)))

     if ("rain_sum" %in% colnames(d)) d <- d[,-which(colnames(d) == 'rain_sum')]

     # ============================================================================
     # ENHANCED COVARIATES INTEGRATION
     # ============================================================================

     message("Adding enhanced covariates for count-based modeling...")

     # 1. Demographics (annual data, interpolated to weekly)
     message("  - Adding demographic variables...")
     demog_path <- file.path(PATHS$DATA_PROCESSED, "demographics", "UN_world_population_prospects_annual.csv")
     if (file.exists(demog_path)) {
          demog_data <- utils::read.csv(demog_path, stringsAsFactors = FALSE)
          demog_data <- demog_data[, c("iso_code", "year", "total_population",
                                       "births_per_1000", "deaths_per_1000")]
          # Filter to MOSAIC countries only
          demog_data <- demog_data[demog_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, demog_data, by = c("iso_code", "year"), all.x = TRUE)
     }

     # 2. World Bank socioeconomic indicators (annual, forward-fill for future years)
     message("  - Adding socioeconomic indicators...")

     # GDP
     gdp_path <- file.path(PATHS$DATA_PROCESSED, "world_bank", "GDP_data_world_bank.csv")
     if (file.exists(gdp_path)) {
          gdp_data <- utils::read.csv(gdp_path, stringsAsFactors = FALSE)
          gdp_data <- gdp_data[, c("iso_code", "year", "GDP")]
          # Filter to MOSAIC countries only
          gdp_data <- gdp_data[gdp_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, gdp_data, by = c("iso_code", "year"), all.x = TRUE)
          # Forward-fill for future years
          d <- d %>%
               dplyr::group_by(iso_code) %>%
               dplyr::mutate(GDP = zoo::na.locf(GDP, na.rm = FALSE)) %>%
               dplyr::ungroup()
     }

     # Population density
     pop_density_path <- file.path(PATHS$DATA_PROCESSED, "world_bank", "population_density_data_world_bank.csv")
     if (file.exists(pop_density_path)) {
          pop_density_data <- utils::read.csv(pop_density_path, stringsAsFactors = FALSE)
          pop_density_data <- pop_density_data[, c("iso_code", "year", "pop_density")]
          names(pop_density_data)[3] <- "population_density"  # Rename for consistency
          # Filter to MOSAIC countries only
          pop_density_data <- pop_density_data[pop_density_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, pop_density_data, by = c("iso_code", "year"), all.x = TRUE)
          d <- d %>%
               dplyr::group_by(iso_code) %>%
               dplyr::mutate(population_density = zoo::na.locf(population_density, na.rm = FALSE)) %>%
               dplyr::ungroup()
     }

     # Urban population
     urban_path <- file.path(PATHS$DATA_PROCESSED, "world_bank", "world_bank_urban_population_data.csv")
     if (file.exists(urban_path)) {
          urban_data <- utils::read.csv(urban_path, stringsAsFactors = FALSE)
          urban_data <- urban_data[, c("iso_code", "year", "urban_pop_prop")]
          names(urban_data)[3] <- "urban_population_pct"  # Rename for consistency
          # Filter to MOSAIC countries only
          urban_data <- urban_data[urban_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, urban_data, by = c("iso_code", "year"), all.x = TRUE)
          d <- d %>%
               dplyr::group_by(iso_code) %>%
               dplyr::mutate(urban_population_pct = zoo::na.locf(urban_population_pct, na.rm = FALSE)) %>%
               dplyr::ungroup()
     }

     # Poverty ratio
     poverty_path <- file.path(PATHS$DATA_PROCESSED, "world_bank", "world_bank_poverty_ratio_data.csv")
     if (file.exists(poverty_path)) {
          poverty_data <- utils::read.csv(poverty_path, stringsAsFactors = FALSE)
          poverty_data <- poverty_data[, c("iso_code", "year", "poverty_ratio")]
          # Filter to MOSAIC countries only
          poverty_data <- poverty_data[poverty_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, poverty_data, by = c("iso_code", "year"), all.x = TRUE)
          d <- d %>%
               dplyr::group_by(iso_code) %>%
               dplyr::mutate(poverty_ratio = zoo::na.locf(poverty_ratio, na.rm = FALSE)) %>%
               dplyr::ungroup()
     }

     # 3. Vaccination coverage (forward-fill to ENSO stop date)
     message("  - Adding vaccination coverage...")
     vax_path <- file.path(PATHS$MODEL_INPUT, "param_nu_vaccination_rate_GTFCC.csv")
     if (file.exists(vax_path)) {
          vax_data <- utils::read.csv(vax_path, stringsAsFactors = FALSE)
          # GTFCC format has 't' column for date and 'j' for iso_code
          vax_data$date <- as.Date(vax_data$t)
          vax_data$iso_code <- vax_data$j
          vax_data$value <- vax_data$parameter_value
          # Convert to weekly aggregates
          vax_data$year <- as.numeric(format(vax_data$date, "%Y"))
          vax_data$week <- lubridate::isoweek(vax_data$date)
          vax_weekly <- vax_data %>%
               dplyr::group_by(iso_code, year, week) %>%
               dplyr::summarise(
                    vaccination_rate_daily = mean(value, na.rm = TRUE),
                    .groups = "drop"
               )
          # Filter to MOSAIC countries only
          vax_weekly <- vax_weekly[vax_weekly$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, vax_weekly, by = c("iso_code", "year", "week"), all.x = TRUE)
          # Forward-fill vaccination rates
          d <- d %>%
               dplyr::group_by(iso_code) %>%
               dplyr::mutate(vaccination_rate_daily = zoo::na.locf(vaccination_rate_daily, na.rm = FALSE)) %>%
               dplyr::ungroup()
          
          # Calculate cumulative vaccine doses administered
          message("    - Calculating cumulative vaccine doses...")
          d <- d %>%
               dplyr::arrange(iso_code, year, week) %>%
               dplyr::group_by(iso_code) %>%
               dplyr::mutate(
                    # Calculate weekly doses (daily rate * 7 days * population)
                    weekly_doses = ifelse(!is.na(vaccination_rate_daily) & !is.na(total_population),
                                         vaccination_rate_daily * 7 * total_population,
                                         0),
                    # Calculate cumulative doses
                    cumulative_vaccine_doses = cumsum(weekly_doses)
               ) %>%
               dplyr::ungroup() %>%
               dplyr::select(-weekly_doses)  # Remove intermediate calculation
     }

     # 4. WASH indicators (static, country-level)
     message("  - Adding WASH indicators...")
     wash_path <- file.path(PATHS$DATA_PROCESSED, "WASH", "WASH_data_Sikder_2023.csv")
     if (file.exists(wash_path)) {
          wash_data <- utils::read.csv(wash_path, stringsAsFactors = FALSE)
          wash_data <- wash_data[, c("iso_code", "Piped_Water", "Other_Improved_Water",
                                    "Septic_or_Sewer_Sanitation", "Other_Improved_Sanitation",
                                    "Unimproved_Water", "Surface_Water",
                                    "Unimproved_Sanitation", "Open_Defecation")]
          # Filter to MOSAIC countries only
          wash_data <- wash_data[wash_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, wash_data, by = "iso_code", all.x = TRUE)

          # Fill NA values with mean of all countries for each WASH variable
          wash_vars <- c("Piped_Water", "Other_Improved_Water", "Septic_or_Sewer_Sanitation",
                        "Other_Improved_Sanitation", "Unimproved_Water", "Surface_Water",
                        "Unimproved_Sanitation", "Open_Defecation")
          for (var in wash_vars) {
               if (var %in% names(d)) {
                    na_count <- sum(is.na(d[[var]]))
                    if (na_count > 0) {
                         mean_val <- mean(d[[var]], na.rm = TRUE)
                         d[[var]][is.na(d[[var]])] <- mean_val
                         message(sprintf("    - Filled %d NA values in %s with mean: %.2f",
                                       na_count, var, mean_val))
                    }
               }
          }
     }

     # 5. Calculate derived climate indices
     message("  - Calculating derived climate indices...")

     # Vapor Pressure Deficit (VPD)
     if (all(c("temperature_2m_mean", "relative_humidity_2m_mean") %in% names(d))) {
          # Saturation vapor pressure (kPa)
          es <- 0.6108 * exp(17.27 * d$temperature_2m_mean / (d$temperature_2m_mean + 237.3))
          # Actual vapor pressure
          ea <- es * (d$relative_humidity_2m_mean / 100)
          # VPD
          d$vpd <- es - ea
     }

     # Moisture Deficit Index
     if (all(c("et0_fao_evapotranspiration_sum", "precipitation_sum") %in% names(d))) {
          d$moisture_deficit <- d$et0_fao_evapotranspiration_sum - d$precipitation_sum
     }

     # Aridity Index
     if (all(c("precipitation_sum", "et0_fao_evapotranspiration_sum") %in% names(d))) {
          d$aridity_index <- ifelse(d$et0_fao_evapotranspiration_sum > 0,
                                    d$precipitation_sum / d$et0_fao_evapotranspiration_sum,
                                    NA)
     }

     # SPEI approximation (standardized by country)
     if (all(c("precipitation_sum", "et0_fao_evapotranspiration_sum") %in% names(d))) {
          d <- d %>%
               dplyr::group_by(iso_code) %>%
               dplyr::mutate(spei_approx = scale(precipitation_sum - et0_fao_evapotranspiration_sum)[,1]) %>%
               dplyr::ungroup()
     }

     # Growing Degree Days (V. cholerae optimal > 15°C)
     if ("temperature_2m_mean" %in% names(d)) {
          d$gdd_cholera <- pmax(0, d$temperature_2m_mean - 15)
     }

     # Diurnal Temperature Range
     if (all(c("temperature_2m_max", "temperature_2m_min") %in% names(d))) {
          d$diurnal_temp_range <- d$temperature_2m_max - d$temperature_2m_min
     }

     # Heat Index (simplified formula)
     if (all(c("temperature_2m_mean", "relative_humidity_2m_mean") %in% names(d))) {
          d$heat_index <- d$temperature_2m_mean +
                        0.348 * d$relative_humidity_2m_mean -
                        0.7 * sqrt(d$relative_humidity_2m_mean) * (d$temperature_2m_mean - 14.4) -
                        2.4
     }

     # 6. Add temporal features
     message("  - Adding temporal features...")

     # Cyclical patterns
     d$sin_annual <- sin(2 * pi * d$week / 52.18)
     d$cos_annual <- cos(2 * pi * d$week / 52.18)
     d$sin_biannual <- sin(4 * pi * d$week / 52.18)
     d$cos_biannual <- cos(4 * pi * d$week / 52.18)
     d$sin_quarterly <- sin(8 * pi * d$week / 52.18)
     d$cos_quarterly <- cos(8 * pi * d$week / 52.18)
     
     # Monthly cyclical patterns
     d$sin_monthly <- sin(2 * pi * d$month / 12)
     d$cos_monthly <- cos(2 * pi * d$month / 12)

     # Temporal trends
     d$time_linear <- scale(as.numeric(d$date))[,1]
     d$years_since_2000 <- pmax(0, d$year - 2000)
     d$years_since_2020 <- pmax(0, d$year - 2020)
     d$epidemic_week <- d$week

     # 7. Add climate interaction terms
     message("  - Adding interaction terms...")

     if (all(c("temperature_2m_mean", "precipitation_sum") %in% names(d))) {
          d$temp_precip_interaction <- d$temperature_2m_mean * d$precipitation_sum
     }

     if (all(c("relative_humidity_2m_mean", "temperature_2m_mean") %in% names(d))) {
          d$humidity_temp_interaction <- d$relative_humidity_2m_mean * d$temperature_2m_mean
     }

     if (all(c("ENSO34", "precipitation_sum") %in% names(d))) {
          d$enso_precip_interaction <- d$ENSO34 * d$precipitation_sum
     }

     if (all(c("DMI", "temperature_2m_mean") %in% names(d))) {
          d$dmi_temp_interaction <- d$DMI * d$temperature_2m_mean
     }

     # Add geographic data (elevation, lat/lon, region)
     message("  - Adding geographic variables...")
     elevation_path <- file.path(PATHS$DATA_ELEVATION, "country_elevation_mean.csv")
     if (file.exists(elevation_path)) {
          elevation_data <- utils::read.csv(elevation_path, stringsAsFactors = FALSE)
          # Filter to MOSAIC countries only
          elevation_data <- elevation_data[elevation_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, elevation_data[c("iso_code", "elevation")], by = "iso_code", all.x = TRUE)
     }
     
     # Add latitude and longitude (country centroids)
     country_coords_path <- file.path(PATHS$DATA_PROCESSED, "geography", "country_centroids.csv")
     if (file.exists(country_coords_path)) {
          coords_data <- utils::read.csv(country_coords_path, stringsAsFactors = FALSE)
          coords_data <- coords_data[, c("iso_code", "latitude", "longitude")]
          # Filter to MOSAIC countries only
          coords_data <- coords_data[coords_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, coords_data, by = "iso_code", all.x = TRUE)
     }
     
     # Add regional classifications (if available)
     region_path <- file.path(PATHS$DATA_PROCESSED, "geography", "country_regions.csv")
     if (file.exists(region_path)) {
          region_data <- utils::read.csv(region_path, stringsAsFactors = FALSE)
          region_data <- region_data[, c("iso_code", "region")]
          # Filter to MOSAIC countries only
          region_data <- region_data[region_data$iso_code %in% iso_codes_mosaic, ]
          d <- merge(d, region_data, by = "iso_code", all.x = TRUE)
     } else {
          # Fallback: create basic regional classification based on ISO codes if file doesn't exist
          message("    - Regional classification file not found, creating basic regions based on WHO regions")
          d$region <- MOSAIC::get_who_region(d$iso_code)
     }

     if (all(c("elevation", "temperature_2m_mean") %in% names(d))) {
          d$elevation_temp_interaction <- d$elevation * d$temperature_2m_mean
     }

     if (all(c("moisture_deficit", "temperature_2m_mean") %in% names(d))) {
          d$moisture_temp_interaction <- d$moisture_deficit * d$temperature_2m_mean
     }

     message(sprintf("Enhanced covariates added. Total columns: %d", ncol(d)))

     # ============================================================================
     # HANDLE REMAINING NA VALUES
     # ============================================================================

     message("Handling NA values in all covariates...")

     # List all numeric columns that might have NAs (exclude identifiers and dates)
     exclude_cols <- c("iso_code", "year", "week", "month", "date", "source",
                      "cases", "deaths", "cases_binary")
     numeric_cols <- names(d)[sapply(d, is.numeric)]
     numeric_cols <- setdiff(numeric_cols, exclude_cols)

     # Separate time series variables from static variables
     timeseries_vars <- c("DMI", "ENSO3", "ENSO34", "ENSO4",
                         "temperature_2m_mean", "temperature_2m_max", "temperature_2m_min",
                         "precipitation_sum", "relative_humidity_2m_mean",
                         "wind_speed_10m_mean", "cloud_cover_mean",
                         "et0_fao_evapotranspiration_sum", "soil_moisture_0_to_10cm_mean",
                         "vpd", "moisture_deficit", "aridity_index", "spei_approx",
                         "gdd_cholera", "diurnal_temp_range", "heat_index",
                         "temp_precip_interaction", "humidity_temp_interaction",
                         "enso_precip_interaction", "dmi_temp_interaction",
                         "elevation_temp_interaction", "moisture_temp_interaction")

     static_vars <- setdiff(numeric_cols, timeseries_vars)

     na_summary <- data.frame(variable = character(), na_count = integer(),
                             method = character(), stringsAsFactors = FALSE)

     # 1. Handle time series variables with interpolation/forward-fill
     message("  - Handling time series variables...")
     for (var in timeseries_vars) {
          if (var %in% names(d)) {
               na_count <- sum(is.na(d[[var]]))
               if (na_count > 0) {
                    # Use last observation carried forward (LOCF) for time series
                    d <- d %>%
                         dplyr::arrange(iso_code, year, week) %>%
                         dplyr::group_by(iso_code) %>%
                         dplyr::mutate(!!var := zoo::na.locf(!!rlang::sym(var), na.rm = FALSE)) %>%
                         dplyr::ungroup()

                    # For any remaining NAs (at the beginning), use next observation carried backward
                    d <- d %>%
                         dplyr::group_by(iso_code) %>%
                         dplyr::mutate(!!var := zoo::na.locf(!!rlang::sym(var), fromLast = TRUE, na.rm = FALSE)) %>%
                         dplyr::ungroup()

                    # If still NAs (entire country missing), use global mean
                    remaining_nas <- sum(is.na(d[[var]]))
                    if (remaining_nas > 0) {
                         mean_val <- mean(d[[var]], na.rm = TRUE)
                         d[[var]][is.na(d[[var]])] <- mean_val
                         na_summary <- rbind(na_summary,
                                           data.frame(variable = var, na_count = na_count,
                                                    method = "interpolate+mean", stringsAsFactors = FALSE))
                    } else {
                         na_summary <- rbind(na_summary,
                                           data.frame(variable = var, na_count = na_count,
                                                    method = "interpolate", stringsAsFactors = FALSE))
                    }
               }
          }
     }

     # 2. Handle static variables with mean imputation
     message("  - Handling static/slow-changing variables...")
     for (col in static_vars) {
          na_count <- sum(is.na(d[[col]]))
          if (na_count > 0) {
               mean_val <- mean(d[[col]], na.rm = TRUE)
               if (!is.na(mean_val)) {
                    d[[col]][is.na(d[[col]])] <- mean_val
                    na_summary <- rbind(na_summary,
                                      data.frame(variable = col, na_count = na_count,
                                               method = "mean", stringsAsFactors = FALSE))
               }
          }
     }

     if (nrow(na_summary) > 0) {
          message(sprintf("  - Handled NA values in %d variables", nrow(na_summary)))
          # Show summary for variables with most NAs
          na_summary <- na_summary[order(na_summary$na_count, decreasing = TRUE), ]
          top_na <- head(na_summary, 5)
          for (i in 1:nrow(top_na)) {
               message(sprintf("    - %s: %d NAs filled using %s",
                             top_na$variable[i], top_na$na_count[i], top_na$method[i]))
          }
          if (nrow(na_summary) > 5) {
               message(sprintf("    - ... and %d more variables", nrow(na_summary) - 5))
          }
     } else {
          message("  - No NA values found in numeric covariates")
     }

     # Final check for any remaining NAs
     total_nas <- sum(is.na(d[, numeric_cols]))
     if (total_nas > 0) {
          warning(sprintf("Warning: %d NA values remain after imputation", total_nas))
     } else {
          message("  - All NA values successfully imputed")
     }

     # Create binary environmental suitability indicator from case data
     message("Creating environmental suitability indicator (cases_binary)...")
     if (use_epidemic_peaks) {
          message("  - Using epidemic peak-based method (deterministic)")
          message("    * Loading epidemic peaks from est_epidemic_peaks() analysis...")
          message("    * Creating deterministic outbreak-based suitability periods...")
          message("    * Using exact peak_start to peak_stop dates without modifications...")
          peak_function <- MOSAIC::get_cases_binary_from_peaks(PATHS)
          d <- peak_function(d)
          message("    * Epidemic peak-based environmental suitability indicator completed!")
     } else {
          message(sprintf("  - Using case threshold method (cutoff = %d)", cutoff))
          d <- MOSAIC::get_cases_binary(d, cutoff = cutoff)
     }

     # ============================================================================
     # ENHANCED TIME SERIES COVARIATES
     # ============================================================================

     message("Creating enhanced time series covariates...")
     
     # Calculate forecast weeks and memory lag for consistent use
     if (forecast_mode) {
          forecast_weeks <- ceiling(forecast_horizon * 4.33)  # Convert months to weeks
          memory_lag <- forecast_weeks + 4  # Add 4-week buffer for forecast safety
          message(sprintf("  - Forecast mode enabled: %d month horizon (%d weeks)", 
                         forecast_horizon, forecast_weeks))
          message(sprintf("  - Using %d-week lag for epidemic memory variables", memory_lag))
          
          # Warning for very long forecast horizons
          if (forecast_horizon > 12) {
               warning(sprintf("Very long forecast horizon (%d months) may reduce reliability of epidemic memory variables", 
                             forecast_horizon))
          }
     } else {
          forecast_weeks <- 0
          memory_lag <- 0  # No additional lag for historical analysis
     }

     # 1. Rolling windows (2, 4, 8, 12 weeks)
     message("  - Adding rolling window variables...")
     d <- d %>%
          dplyr::group_by(iso_code) %>%
          dplyr::arrange(date, .by_group = TRUE) %>%
          dplyr::mutate(
               # Precipitation rolling windows
               precip_sum_2w = slider::slide_dbl(precipitation_sum, sum, .before = 1, .complete = TRUE),
               precip_sum_4w = slider::slide_dbl(precipitation_sum, sum, .before = 3, .complete = TRUE),
               precip_sum_8w = slider::slide_dbl(precipitation_sum, sum, .before = 7, .complete = TRUE),
               precip_sum_12w = slider::slide_dbl(precipitation_sum, sum, .before = 11, .complete = TRUE),
               
               # Temperature rolling windows
               temp_mean_2w = slider::slide_dbl(temperature_2m_mean, mean, .before = 1, .complete = TRUE),
               temp_mean_4w = slider::slide_dbl(temperature_2m_mean, mean, .before = 3, .complete = TRUE),
               temp_mean_8w = slider::slide_dbl(temperature_2m_mean, mean, .before = 7, .complete = TRUE),
               temp_mean_12w = slider::slide_dbl(temperature_2m_mean, mean, .before = 11, .complete = TRUE),
               
               # Relative humidity rolling windows
               rh_mean_2w = slider::slide_dbl(relative_humidity_2m_mean, mean, .before = 1, .complete = TRUE),
               rh_mean_4w = slider::slide_dbl(relative_humidity_2m_mean, mean, .before = 3, .complete = TRUE),
               rh_mean_8w = slider::slide_dbl(relative_humidity_2m_mean, mean, .before = 7, .complete = TRUE),
               rh_mean_12w = slider::slide_dbl(relative_humidity_2m_mean, mean, .before = 11, .complete = TRUE)
          ) %>%
          dplyr::ungroup()

     # 2. Standardized anomalies vs local climatology (by location × week-of-year)
     message("  - Calculating standardized anomalies...")
     d <- d %>%
          dplyr::group_by(iso_code, week) %>%
          dplyr::mutate(
               # Precipitation anomalies
               precip_clim_mean = mean(precipitation_sum, na.rm = TRUE),
               precip_clim_sd = sd(precipitation_sum, na.rm = TRUE),
               precip_anom = (precipitation_sum - precip_clim_mean) / ifelse(precip_clim_sd > 0, precip_clim_sd, 1),
               
               # Temperature anomalies
               temp_clim_mean = mean(temperature_2m_mean, na.rm = TRUE),
               temp_clim_sd = sd(temperature_2m_mean, na.rm = TRUE),
               temp_anom = (temperature_2m_mean - temp_clim_mean) / ifelse(temp_clim_sd > 0, temp_clim_sd, 1),
               
               # VPD anomalies (if available)
               vpd_clim_mean = ifelse("vpd" %in% names(.), mean(vpd, na.rm = TRUE), NA),
               vpd_clim_sd = ifelse("vpd" %in% names(.), sd(vpd, na.rm = TRUE), NA),
               vpd_anom = ifelse("vpd" %in% names(.), 
                               (vpd - vpd_clim_mean) / ifelse(vpd_clim_sd > 0, vpd_clim_sd, 1), 
                               NA),
               
               # Soil moisture anomalies
               soilm_clim_mean = mean(soil_moisture_0_to_10cm_mean, na.rm = TRUE),
               soilm_clim_sd = sd(soil_moisture_0_to_10cm_mean, na.rm = TRUE),
               soil_moisture_anom = (soil_moisture_0_to_10cm_mean - soilm_clim_mean) / 
                                  ifelse(soilm_clim_sd > 0, soilm_clim_sd, 1)
          ) %>%
          # Remove intermediate climatology columns
          dplyr::select(-precip_clim_mean, -precip_clim_sd, -temp_clim_mean, -temp_clim_sd,
                       -vpd_clim_mean, -vpd_clim_sd, -soilm_clim_mean, -soilm_clim_sd) %>%
          dplyr::ungroup()

     # 3. Climate extremes and spell indicators
     message("  - Adding climate extremes indicators...")
     d <- d %>%
          dplyr::group_by(iso_code) %>%
          dplyr::arrange(date, .by_group = TRUE) %>%
          dplyr::mutate(
               # Precipitation extremes (using 90th percentile threshold)
               precip_p90_threshold = quantile(precipitation_sum, 0.9, na.rm = TRUE),
               precip_extreme_p90_count = as.numeric(precipitation_sum >= precip_p90_threshold),
               
               # Temperature extremes (heatwave proxy - above 95th percentile)
               temp_p95_threshold = quantile(temperature_2m_mean, 0.95, na.rm = TRUE),
               heatwave_days = as.numeric(temperature_2m_mean >= temp_p95_threshold),
               
               # Dry spell length (consecutive weeks with precip < 1mm)
               dry_week = precipitation_sum < 1.0,
               dry_spell_len = slider::slide_dbl(dry_week, ~ sum(.x), .before = 6, .complete = TRUE)
          ) %>%
          dplyr::select(-precip_p90_threshold, -temp_p95_threshold, -dry_week) %>%
          dplyr::ungroup()

     # 4. Epidemic memory variables (always included, with forecast-safe lags)
     message("  - Adding epidemic memory variables...")
     
     d <- d %>%
          dplyr::group_by(iso_code) %>%
          dplyr::arrange(date, .by_group = TRUE) %>%
          dplyr::mutate(
               # Growth rate (log difference) - robust to NAs
               r_t = {
                    cases_lagged <- dplyr::lag(cases, 1 + memory_lag)
                    cases_current <- dplyr::lag(cases, memory_lag)
                    ifelse(is.na(cases_lagged) | is.na(cases_current), 
                          NA_real_, 
                          log1p(cases_current) - log1p(cases_lagged))
               },
               
               # Cumulative cases in rolling windows - robust to NAs
               cum_cases_4w = {
                    cases_lagged <- dplyr::lag(cases, memory_lag)
                    slider::slide_dbl(cases_lagged, ~ sum(.x, na.rm = TRUE), 
                                     .before = 3, .complete = FALSE)
               },
               cum_cases_8w = {
                    cases_lagged <- dplyr::lag(cases, memory_lag)
                    slider::slide_dbl(cases_lagged, ~ sum(.x, na.rm = TRUE), 
                                     .before = 7, .complete = FALSE)
               },
               cum_cases_12w = {
                    cases_lagged <- dplyr::lag(cases, memory_lag)
                    slider::slide_dbl(cases_lagged, ~ sum(.x, na.rm = TRUE), 
                                     .before = 11, .complete = FALSE)
               },
               cum_cases_52w = {
                    cases_lagged <- dplyr::lag(cases, memory_lag)
                    slider::slide_dbl(cases_lagged, ~ sum(.x, na.rm = TRUE), 
                                     .before = 51, .complete = FALSE)
               },
               
               # Surveillance intensity proxy - robust to NAs
               nonzero_ratio_52w = {
                    cases_lagged <- dplyr::lag(cases, memory_lag)
                    cases_binary <- ifelse(is.na(cases_lagged), FALSE, cases_lagged > 0)
                    slider::slide_dbl(cases_binary, ~ mean(.x, na.rm = TRUE), 
                                     .before = 51, .complete = FALSE)
               },
               
               # Weeks since last significant outbreak (≥100 cases for major outbreaks)
               weeks_since_major_outbreak = {
                    cases_lagged <- dplyr::lag(cases, memory_lag)
                    major_outbreak <- ifelse(is.na(cases_lagged), FALSE, cases_lagged >= 100)
                    weeks_since <- integer(length(major_outbreak))
                    counter <- 0
                    for (i in seq_along(major_outbreak)) {
                         if (!is.na(major_outbreak[i]) && major_outbreak[i]) {
                              counter <- 0
                         } else {
                              counter <- counter + 1
                         }
                         weeks_since[i] <- counter
                    }
                    weeks_since
               },
               
               # Historical outbreak intensity (peak cases in past year)
               peak_cases_52w = {
                    cases_lagged <- dplyr::lag(cases, memory_lag)
                    slider::slide_dbl(cases_lagged, ~ {
                         valid_cases <- .x[!is.na(.x)]
                         if (length(valid_cases) > 0) max(valid_cases) else NA_real_
                    }, .before = 51, .complete = FALSE)
               },
               
               # Outbreak seasonality indicator (typical outbreak timing)
               seasonal_outbreak_risk = {
                    cases_lagged <- dplyr::lag(cases, memory_lag)
                    # Calculate historical mean cases for this week-of-year
                    mean(cases_lagged, na.rm = TRUE)
               }
          ) %>%
          dplyr::ungroup()

     # 5. Enhanced Spatial Import Variables
     message("  - Adding enhanced spatial import variables...")
     
     # Load mobility matrices
     pi_diffusion_path <- file.path(PATHS$MODEL_INPUT, "param_pi_diffusion.csv")
     tau_departure_path <- file.path(PATHS$MODEL_INPUT, "param_tau_departure.csv")
     
     # Check for required mobility data files
     mobility_files_exist <- file.exists(pi_diffusion_path) && file.exists(tau_departure_path)
     
     if (mobility_files_exist) {
          # Load π (diffusion/connectivity) matrix data
          pi_data <- utils::read.csv(pi_diffusion_path, stringsAsFactors = FALSE)
          
          # Load τ (departure rate) data
          tau_data <- utils::read.csv(tau_departure_path, stringsAsFactors = FALSE)
          tau_point <- tau_data[tau_data$parameter_distribution == "point", c("i", "parameter_value")]
          names(tau_point) <- c("iso_code", "tau_departure")
          
          # Get countries available in both mobility data and main dataset
          pi_countries <- sort(unique(c(pi_data$i, pi_data$j)))
          tau_countries <- sort(unique(tau_point$iso_code))
          available_countries <- intersect(intersect(pi_countries, tau_countries), 
                                         intersect(unique(d$iso_code), iso_codes_mosaic))
          
          if (length(available_countries) >= 10) {  # Minimum threshold for meaningful spatial analysis
               message(sprintf("    - Processing spatial variables for %d countries", length(available_countries)))
               
               # ============================================================================
               # STRUCTURAL SPATIAL VARIABLES (Time-Invariant, Fully Forecast-Safe)
               # ============================================================================
               
               message("      * Adding structural spatial variables...")
               
               # Create π matrix (destination × origin format)
               pi_matrix <- matrix(0, nrow = length(available_countries), ncol = length(available_countries))
               rownames(pi_matrix) <- available_countries
               colnames(pi_matrix) <- available_countries
               
               # Fill π matrix with mobility probabilities
               for (idx in 1:nrow(pi_data)) {
                    dest <- pi_data$i[idx]      # Destination country
                    origin <- pi_data$j[idx]    # Origin country  
                    pi_value <- pi_data$parameter_value[idx]
                    
                    if (dest %in% available_countries && origin %in% available_countries && !is.na(pi_value)) {
                         pi_matrix[dest, origin] <- pi_value
                    }
               }
               
               # Create τ vector for departure rates
               tau_vector <- setNames(tau_point$tau_departure[match(available_countries, tau_point$iso_code)], 
                                    available_countries)
               tau_vector[is.na(tau_vector)] <- median(tau_vector, na.rm = TRUE)  # Fill missing with median
               
               # Calculate structural spatial variables for each country
               structural_spatial_vars <- data.frame(
                    iso_code = available_countries,
                    stringsAsFactors = FALSE
               )
               
               # 1. Import Vulnerability Index: Σ_j π_ij (total inward mobility potential)
               structural_spatial_vars$import_vulnerability <- rowSums(pi_matrix, na.rm = TRUE)
               
               # 2. Export Potential Index: τ_i (departure rate capacity)
               structural_spatial_vars$export_potential <- tau_vector[available_countries]
               
               # 3. Weighted Import Connectivity: Σ_j (π_ij × τ_j) (most epidemiologically meaningful)
               weighted_connectivity <- numeric(length(available_countries))
               names(weighted_connectivity) <- available_countries
               
               for (i in seq_along(available_countries)) {
                    dest_country <- available_countries[i]
                    connectivity_sum <- 0
                    
                    for (j in seq_along(available_countries)) {
                         origin_country <- available_countries[j]
                         if (dest_country != origin_country) {
                              pi_ij <- pi_matrix[dest_country, origin_country]
                              tau_j <- tau_vector[origin_country]
                              if (!is.na(pi_ij) && !is.na(tau_j)) {
                                   connectivity_sum <- connectivity_sum + (pi_ij * tau_j)
                              }
                         }
                    }
                    weighted_connectivity[dest_country] <- connectivity_sum
               }
               structural_spatial_vars$weighted_import_connectivity <- weighted_connectivity[available_countries]
               
               # 4. Connectivity Degree: Count of significant mobility connections (threshold: >1e-5)
               connection_threshold <- 1e-5
               structural_spatial_vars$connectivity_degree <- rowSums(pi_matrix > connection_threshold, na.rm = TRUE)
               
               # 5. Betweenness Centrality: Network position as epidemic intermediary
               # Simplified calculation: countries that are highly connected to other highly connected countries
               node_strength <- rowSums(pi_matrix, na.rm = TRUE) + colSums(pi_matrix, na.rm = TRUE)
               betweenness <- numeric(length(available_countries))
               names(betweenness) <- available_countries
               
               for (i in seq_along(available_countries)) {
                    country <- available_countries[i]
                    # Betweenness approximated as sum of (π_ij × strength_j) across all connections
                    betweenness_sum <- 0
                    for (j in seq_along(available_countries)) {
                         if (i != j) {
                              pi_ij <- pi_matrix[country, available_countries[j]]
                              pi_ji <- pi_matrix[available_countries[j], country]
                              strength_j <- node_strength[available_countries[j]]
                              betweenness_sum <- betweenness_sum + ((pi_ij + pi_ji) * strength_j)
                         }
                    }
                    betweenness[country] <- betweenness_sum
               }
               structural_spatial_vars$betweenness_centrality <- betweenness[available_countries]
               
               # 6. Eigenvector Centrality: Connectivity importance score
               # Simplified: weighted average of connections to important nodes
               eigenvector_centrality <- numeric(length(available_countries))
               names(eigenvector_centrality) <- available_countries
               
               for (i in seq_along(available_countries)) {
                    country <- available_countries[i]
                    importance_sum <- 0
                    total_connections <- 0
                    
                    for (j in seq_along(available_countries)) {
                         if (i != j) {
                              pi_ij <- pi_matrix[country, available_countries[j]]
                              pi_ji <- pi_matrix[available_countries[j], country]
                              neighbor_strength <- node_strength[available_countries[j]]
                              
                              if (!is.na(pi_ij) && pi_ij > 0) {
                                   importance_sum <- importance_sum + (pi_ij * neighbor_strength)
                                   total_connections <- total_connections + pi_ij
                              }
                              if (!is.na(pi_ji) && pi_ji > 0) {
                                   importance_sum <- importance_sum + (pi_ji * neighbor_strength)
                                   total_connections <- total_connections + pi_ji
                              }
                         }
                    }
                    
                    eigenvector_centrality[country] <- if (total_connections > 0) {
                         importance_sum / total_connections
                    } else {
                         0
                    }
               }
               structural_spatial_vars$eigenvector_centrality <- eigenvector_centrality[available_countries]
               
               # 7. Import-Export Balance: τ_i / Σ_j π_ij (ratio of export to import potential)
               structural_spatial_vars$import_export_balance <- ifelse(structural_spatial_vars$import_vulnerability > 0,
                    structural_spatial_vars$export_potential / structural_spatial_vars$import_vulnerability,
                    structural_spatial_vars$export_potential)  # If no imports, just export potential
               
               # Merge structural variables (broadcast to all time periods)
               d <- merge(d, structural_spatial_vars, by = "iso_code", all.x = TRUE)
               
               message("      * Added 7 structural spatial variables (forecast-safe)")
               
               # ============================================================================
               # CASE-BASED SPATIAL VARIABLES (Time-Varying, Limited Forecast Horizon)
               # ============================================================================
               
               message("      * Skipping case-based spatial import variables (removed for performance)")
               
          } else {
               message(sprintf("    - Insufficient countries with mobility data (%d), skipping spatial variables", 
                              length(available_countries)))
          }
     } else {
          missing_files <- c()
          if (!file.exists(pi_diffusion_path)) missing_files <- c(missing_files, "param_pi_diffusion.csv")
          if (!file.exists(tau_departure_path)) missing_files <- c(missing_files, "param_tau_departure.csv")
          message(sprintf("    - Missing mobility data files (%s), skipping spatial variables", 
                         paste(missing_files, collapse = ", ")))
     }

     # 6. Enhanced interaction terms (WASH × extremes, Urban × climate, nonlinear transforms)
     message("  - Adding enhanced interaction terms...")
     
     # WASH × extreme precipitation
     if (all(c("precip_extreme_p90_count", "Unimproved_Sanitation") %in% names(d))) {
          d$wash_precip_extreme_interaction <- d$precip_extreme_p90_count * d$Unimproved_Sanitation
     }
     
     # Urban × precipitation anomaly
     if (all(c("urban_population_pct", "precip_anom") %in% names(d))) {
          d$urban_precip_anom_interaction <- d$urban_population_pct * d$precip_anom
     }
     
     # Nonlinear transforms for key variables
     if ("precip_anom" %in% names(d)) {
          d$precip_anom_sq <- d$precip_anom^2
     }
     
     if ("temp_anom" %in% names(d)) {
          d$temp_anom_sq <- d$temp_anom^2
     }
     
     # Log transform for skewed variables (cases already handled as log1p in r_t)
     if ("cumulative_vaccine_doses" %in% names(d)) {
          d$log1p_cum_vaccine_doses <- log1p(d$cumulative_vaccine_doses)
     }

     # 7. Time-lagged climate variables (epidemiologically-informed lags)
     if (include_lags) {
          message("  - Adding time-lagged climate variables...")
          
          d <- d %>%
               dplyr::group_by(iso_code) %>%
               dplyr::arrange(date, .by_group = TRUE) %>%
               dplyr::mutate(
                    # Precipitation: 1,2,4,6,8 weeks (runoff/flooding → contamination)
                    precipitation_sum_lag1 = dplyr::lag(precipitation_sum, 1),
                    precipitation_sum_lag2 = dplyr::lag(precipitation_sum, 2),
                    precipitation_sum_lag4 = dplyr::lag(precipitation_sum, 4),
                    precipitation_sum_lag6 = dplyr::lag(precipitation_sum, 6),    # Optimal secondary period (r = 0.029)
                    precipitation_sum_lag8 = dplyr::lag(precipitation_sum, 8),
                    
                    # Temperature: 2,4,8 weeks (growth/survival of vibrios)
                    temperature_2m_mean_lag2 = dplyr::lag(temperature_2m_mean, 2),
                    temperature_2m_mean_lag4 = dplyr::lag(temperature_2m_mean, 4),
                    temperature_2m_mean_lag8 = dplyr::lag(temperature_2m_mean, 8),
                    temperature_2m_max_lag2 = dplyr::lag(temperature_2m_max, 2),
                    temperature_2m_max_lag4 = dplyr::lag(temperature_2m_max, 4),
                    temperature_2m_max_lag6 = dplyr::lag(temperature_2m_max, 6),    # Consistent negative correlation (r = -0.020)
                    temperature_2m_max_lag8 = dplyr::lag(temperature_2m_max, 8),
                    
                    # Relative humidity: 1,2,4 weeks (survival; interacts with heat)
                    relative_humidity_2m_mean_lag1 = dplyr::lag(relative_humidity_2m_mean, 1),
                    relative_humidity_2m_mean_lag2 = dplyr::lag(relative_humidity_2m_mean, 2),
                    relative_humidity_2m_mean_lag4 = dplyr::lag(relative_humidity_2m_mean, 4),
                    
                    # VPD: 1,4,8 weeks (dryness preceding wet periods)
                    # Only create meaningful lags if vpd exists and has valid data
                    vpd_lag1 = if("vpd" %in% names(.) && !all(is.na(vpd))) dplyr::lag(vpd, 1) else NA_real_,
                    vpd_lag4 = if("vpd" %in% names(.) && !all(is.na(vpd))) dplyr::lag(vpd, 4) else NA_real_,
                    vpd_lag8 = if("vpd" %in% names(.) && !all(is.na(vpd))) dplyr::lag(vpd, 8) else NA_real_,
                    
                    # Soil moisture: 1,2,4,6,8 weeks (environmental reservoir readiness)
                    soil_moisture_0_to_10cm_mean_lag1 = dplyr::lag(soil_moisture_0_to_10cm_mean, 1),
                    soil_moisture_0_to_10cm_mean_lag2 = dplyr::lag(soil_moisture_0_to_10cm_mean, 2),    # Optimal period (r = 0.043)
                    soil_moisture_0_to_10cm_mean_lag4 = dplyr::lag(soil_moisture_0_to_10cm_mean, 4),
                    soil_moisture_0_to_10cm_mean_lag6 = dplyr::lag(soil_moisture_0_to_10cm_mean, 6),    # Secondary optimal (r = 0.037)
                    soil_moisture_0_to_10cm_mean_lag8 = dplyr::lag(soil_moisture_0_to_10cm_mean, 8),
                    
                    # SPEI: 4,8,12 weeks (integrated hydro-climate balance)
                    # Only create meaningful lags if spei_approx exists and has valid data
                    spei_approx_lag4 = if("spei_approx" %in% names(.) && !all(is.na(spei_approx))) dplyr::lag(spei_approx, 4) else NA_real_,
                    spei_approx_lag8 = if("spei_approx" %in% names(.) && !all(is.na(spei_approx))) dplyr::lag(spei_approx, 8) else NA_real_,
                    spei_approx_lag12 = if("spei_approx" %in% names(.) && !all(is.na(spei_approx))) dplyr::lag(spei_approx, 12) else NA_real_,
                    
                    # Evapotranspiration: 1,4,8 weeks
                    et0_fao_evapotranspiration_sum_lag1 = dplyr::lag(et0_fao_evapotranspiration_sum, 1),
                    et0_fao_evapotranspiration_sum_lag4 = dplyr::lag(et0_fao_evapotranspiration_sum, 4),
                    et0_fao_evapotranspiration_sum_lag8 = dplyr::lag(et0_fao_evapotranspiration_sum, 8),
                    
                    # Diurnal temperature range: 2,4 weeks
                    # Only create meaningful lags if diurnal_temp_range exists and has valid data
                    diurnal_temp_range_lag2 = if("diurnal_temp_range" %in% names(.) && !all(is.na(diurnal_temp_range)))
                                                    dplyr::lag(diurnal_temp_range, 2) else NA_real_,
                    diurnal_temp_range_lag4 = if("diurnal_temp_range" %in% names(.) && !all(is.na(diurnal_temp_range)))
                                                    dplyr::lag(diurnal_temp_range, 4) else NA_real_,
                    
                    # Heat index: 2,4,8 weeks
                    # Only create meaningful lags if heat_index exists and has valid data
                    heat_index_lag2 = if("heat_index" %in% names(.) && !all(is.na(heat_index))) dplyr::lag(heat_index, 2) else NA_real_,
                    heat_index_lag4 = if("heat_index" %in% names(.) && !all(is.na(heat_index))) dplyr::lag(heat_index, 4) else NA_real_,
                    heat_index_lag8 = if("heat_index" %in% names(.) && !all(is.na(heat_index))) dplyr::lag(heat_index, 8) else NA_real_,
                    
                    # Cloud cover: 1,4 weeks
                    cloud_cover_mean_lag1 = dplyr::lag(cloud_cover_mean, 1),
                    cloud_cover_mean_lag4 = dplyr::lag(cloud_cover_mean, 4),
                    
                    # Wind speed: 1,2 weeks (often weak, try sparingly)
                    wind_speed_10m_mean_lag1 = dplyr::lag(wind_speed_10m_mean, 1),
                    wind_speed_10m_mean_lag2 = dplyr::lag(wind_speed_10m_mean, 2),
                    
                    # Large-scale modes: Evidence-based optimal lags from correlation analysis
                    # ENSO3 (Eastern Pacific) - optimal at 4-6 months (16-24 weeks)
                    ENSO3_lag4 = dplyr::lag(ENSO3, 4),      # 1 month - good performance
                    ENSO3_lag16 = dplyr::lag(ENSO3, 16),    # 4 months - strong correlation
                    ENSO3_lag20 = dplyr::lag(ENSO3, 20),    # 5 months - peak correlation (0.216)
                    ENSO3_lag24 = dplyr::lag(ENSO3, 24),    # 6 months - optimal period (0.218)

                    # ENSO34 (Central Pacific) - optimal at 6 and 11 months
                    ENSO34_lag6 = dplyr::lag(ENSO34, 6),    # Early response
                    ENSO34_lag24 = dplyr::lag(ENSO34, 24),  # 6 months - secondary peak (0.193)
                    ENSO34_lag44 = dplyr::lag(ENSO34, 44),  # 11 months - optimal (0.203)

                    # ENSO4 (Western Pacific) - optimal at 10-11 months
                    ENSO4_lag40 = dplyr::lag(ENSO4, 40),    # 10 months - near-optimal (0.226)
                    ENSO4_lag44 = dplyr::lag(ENSO4, 44),    # 11 months - peak correlation (0.232)

                    # DMI (Indian Ocean Dipole) - stable across 4-5 months, some immediate effects
                    DMI_lag0 = DMI,                         # Immediate effects (strong in some countries)
                    DMI_lag4 = dplyr::lag(DMI, 4),         # 1 month - stable period
                    DMI_lag16 = dplyr::lag(DMI, 16),       # 4 months - good performance
                    DMI_lag20 = dplyr::lag(DMI, 20)        # 5 months - optimal period (0.185)
               ) %>%
               dplyr::ungroup()
          
          message(sprintf("    - Added %d time-lagged climate variables (including immediate effects)", 51))

          # Remove any lag columns that are entirely NA (failed conditional creation)
          lag_columns <- names(d)[grepl("_lag\\d+$", names(d))]
          all_na_lags <- sapply(d[lag_columns], function(x) all(is.na(x)))
          if (any(all_na_lags)) {
               removed_lags <- names(all_na_lags)[all_na_lags]
               d <- d[, !names(d) %in% removed_lags]
               message(sprintf("    - Removed %d all-NA lag columns: %s",
                              length(removed_lags),
                              paste(head(removed_lags, 5), collapse = ", ")))
          }
     } else {
          message("  - Skipping time-lagged climate variables")
     }

     message(sprintf("Enhanced covariates added. Total columns: %d", ncol(d)))

     # Remove source column if it exists
     if ("source" %in% names(d)) {
          d <- d[, !names(d) %in% "source"]
     }

     # Reorder columns thematically (matching the order of data assembly)
     # 1. Location/Time/Cases/Deaths
     base_cols <- c("iso_code", "year", "month", "week", "date", "cases", "cases_binary", "deaths")
     
     # 2. Temporal variables
     temporal_cols <- c("sin_annual", "cos_annual", "sin_biannual", "cos_biannual", 
                       "sin_quarterly", "cos_quarterly", "sin_monthly", "cos_monthly",
                       "time_linear", "years_since_2000", "years_since_2020", "epidemic_week")
     
     # 3. Demographics and socioeconomic (moved before ENSO)
     demo_cols <- c("total_population", "births_per_1000", "deaths_per_1000",
                   "GDP", "population_density", "urban_population_pct", "poverty_ratio")
     
     # 4. Vaccination and other health (moved before ENSO)
     health_cols <- c("vaccination_rate_daily", "cumulative_vaccine_doses")
     
     # 5. WASH indicators (moved before ENSO)
     wash_cols <- c("Piped_Water", "Other_Improved_Water", "Septic_or_Sewer_Sanitation",
                   "Other_Improved_Sanitation", "Unimproved_Water", "Surface_Water",
                   "Unimproved_Sanitation", "Open_Defecation")
     
     # 6. ENSO/Climate indices
     enso_cols <- c("DMI", "ENSO3", "ENSO34", "ENSO4")
     
     # 7. Basic climate variables
     basic_climate_cols <- c("temperature_2m_mean", "temperature_2m_max", "temperature_2m_min",
                            "precipitation_sum", "relative_humidity_2m_mean", "wind_speed_10m_mean",
                            "cloud_cover_mean", "et0_fao_evapotranspiration_sum", 
                            "soil_moisture_0_to_10cm_mean")
     
     # 8. Rolling window variables
     rolling_cols <- c("precip_sum_2w", "precip_sum_4w", "precip_sum_8w", "precip_sum_12w",
                      "temp_mean_2w", "temp_mean_4w", "temp_mean_8w", "temp_mean_12w", 
                      "rh_mean_2w", "rh_mean_4w", "rh_mean_8w", "rh_mean_12w")
     
     # 9. Standardized anomalies
     anomaly_cols <- c("precip_anom", "temp_anom", "vpd_anom", "soil_moisture_anom")
     
     # 10. Climate extremes and spells  
     extremes_cols <- c("precip_extreme_p90_count", "heatwave_days", "dry_spell_len")
     
     # 11. Epidemic memory variables (always included, with forecast-safe lags)
     memory_cols <- c("r_t", "cum_cases_4w", "cum_cases_8w", "cum_cases_12w", "cum_cases_52w",
                     "nonzero_ratio_52w", "weeks_since_major_outbreak", 
                     "peak_cases_52w", "seasonal_outbreak_risk")
     
     # 12. Enhanced Spatial Import Variables
     # Structural variables (always included - forecast-safe)
     structural_spatial_cols <- c("import_vulnerability", "export_potential", "weighted_import_connectivity", 
                                 "connectivity_degree", "betweenness_centrality", "eigenvector_centrality", 
                                 "import_export_balance")
     
     # Case-based variables removed for performance
     case_based_spatial_cols <- c()
     
     # Combine all spatial variables
     spatial_cols <- c(structural_spatial_cols, case_based_spatial_cols)
     
     # 13. Climate interaction terms (original)
     interaction_cols <- c("temp_precip_interaction", "humidity_temp_interaction",
                          "enso_precip_interaction", "dmi_temp_interaction",
                          "elevation_temp_interaction", "moisture_temp_interaction")
     
     # 14. Enhanced interaction terms (WASH × extremes, Urban × climate) 
     enhanced_interaction_cols <- c("wash_precip_extreme_interaction", "urban_precip_anom_interaction")
     
     # 15. Nonlinear transforms
     nonlinear_cols <- c("precip_anom_sq", "temp_anom_sq", "log1p_cum_vaccine_doses")
     
     # 16. Time-lagged climate variables (if included)
     if (include_lags) {
          lag_cols <- c("precipitation_sum_lag1", "precipitation_sum_lag2", "precipitation_sum_lag4", "precipitation_sum_lag6", "precipitation_sum_lag8",
                       "temperature_2m_mean_lag2", "temperature_2m_mean_lag4", "temperature_2m_mean_lag8",
                       "temperature_2m_max_lag2", "temperature_2m_max_lag4", "temperature_2m_max_lag6", "temperature_2m_max_lag8",
                       "relative_humidity_2m_mean_lag1", "relative_humidity_2m_mean_lag2", "relative_humidity_2m_mean_lag4",
                       "vpd_lag1", "vpd_lag4", "vpd_lag8",
                       "soil_moisture_0_to_10cm_mean_lag1", "soil_moisture_0_to_10cm_mean_lag2", "soil_moisture_0_to_10cm_mean_lag4", "soil_moisture_0_to_10cm_mean_lag6", "soil_moisture_0_to_10cm_mean_lag8",
                       "spei_approx_lag4", "spei_approx_lag8", "spei_approx_lag12",
                       "et0_fao_evapotranspiration_sum_lag1", "et0_fao_evapotranspiration_sum_lag4", "et0_fao_evapotranspiration_sum_lag8",
                       "diurnal_temp_range_lag2", "diurnal_temp_range_lag4",
                       "heat_index_lag2", "heat_index_lag4", "heat_index_lag8",
                       "cloud_cover_mean_lag1", "cloud_cover_mean_lag4",
                       "wind_speed_10m_mean_lag1", "wind_speed_10m_mean_lag2",
                       "ENSO3_lag4", "ENSO3_lag16", "ENSO3_lag20", "ENSO3_lag24",
                       "ENSO34_lag6", "ENSO34_lag24", "ENSO34_lag44",
                       "ENSO4_lag40", "ENSO4_lag44",
                       "DMI_lag0", "DMI_lag4", "DMI_lag16", "DMI_lag20")
     } else {
          lag_cols <- c()
     }
     
     # 17. Derived climate indices
     indices_cols <- c("vpd", "moisture_deficit", "aridity_index", "spei_approx",
                      "gdd_cholera", "diurnal_temp_range", "heat_index")
     
     # 18. Geographic
     geo_cols <- c("elevation", "latitude", "longitude", "region")
     
     # Combine all organized columns
     organized_cols <- c(base_cols, temporal_cols, demo_cols, health_cols, wash_cols,
                        enso_cols, basic_climate_cols, rolling_cols, anomaly_cols,
                        extremes_cols, memory_cols, spatial_cols, interaction_cols, 
                        enhanced_interaction_cols, nonlinear_cols, lag_cols, indices_cols, geo_cols)
     
     # Get any remaining columns not explicitly categorized
     remaining_cols <- setdiff(names(d), organized_cols)
     
     # Final column order
     final_cols <- c(organized_cols, remaining_cols)
     
     # Only keep columns that actually exist in the data
     final_cols <- final_cols[final_cols %in% names(d)]
     
     d <- d[, final_cols]
     
     # Final validation message for forecast mode
     if (forecast_mode) {
          # Count spatial variable categories
          structural_spatial_present <- sum(structural_spatial_cols %in% names(d))
          case_based_spatial_present <- sum(case_based_spatial_cols %in% names(d))
          lag_adjusted_memory <- c("r_t", "cum_cases_4w", "cum_cases_8w", "cum_cases_12w", "cum_cases_52w",
                                  "nonzero_ratio_52w", "weeks_since_major_outbreak")
          memory_present <- sum(lag_adjusted_memory %in% names(d))
          
          message(sprintf("  - Forecast mode validated: dataset ready for %d-month forecasting", forecast_horizon))
          message(sprintf("  - Epidemic memory variables: %d included with %d-week lag", 
                         memory_present, memory_lag))
          message(sprintf("  - Structural spatial variables: %d included (forecast-safe)", structural_spatial_present))
          if (case_based_spatial_present > 0) {
               if (forecast_horizon > 1) {
                    message(sprintf("  - Case-based spatial variables: %d included with reliability warning", case_based_spatial_present))
               } else {
                    message(sprintf("  - Case-based spatial variables: %d included", case_based_spatial_present))
               }
          }
     }

     # Remove unnecessary objects from memory
     rm(cases_data, combined_climate_data, combined_climate_data_wide, enso_data, enso_data_wide)

     if (53 %in% d$week) stop("week index is out of bounds")

     # Generate comprehensive dataset statistics
     message("=== DATASET SUMMARY ===")
     
     # Basic dimensions
     n_countries <- length(unique(d$iso_code))
     n_observations <- nrow(d)
     n_covariates <- ncol(d) - 8  # Subtract base columns (iso_code, year, month, week, date, cases, cases_binary, deaths)
     
     # Date ranges
     # Overall dataset (covariates)
     covariate_date_start <- min(d$date, na.rm = TRUE)
     covariate_date_end <- max(d$date, na.rm = TRUE)
     covariate_span_years <- round(as.numeric(difftime(covariate_date_end, covariate_date_start, units = "days")) / 365.25, 1)
     
     # Observed case data (non-NA, non-zero cases only)
     case_data_dates <- d$date[!is.na(d$cases) & d$cases > 0]
     if (length(case_data_dates) > 0) {
          case_date_start <- min(case_data_dates)
          case_date_end <- max(case_data_dates)
          case_span_years <- round(as.numeric(difftime(case_date_end, case_date_start, units = "days")) / 365.25, 1)
     } else {
          case_date_start <- NA
          case_date_end <- NA
          case_span_years <- 0
     }
     
     # Countries list (abbreviated if many)
     countries_list <- sort(unique(d$iso_code))
     if (length(countries_list) <= 10) {
          countries_str <- paste(countries_list, collapse = ", ")
     } else {
          countries_str <- paste(paste(countries_list[1:8], collapse = ", "), 
                               sprintf("... and %d more", length(countries_list) - 8))
     }
     
     # Case statistics
     total_cases <- sum(d$cases, na.rm = TRUE)
     total_deaths <- sum(d$deaths, na.rm = TRUE)
     case_weeks <- sum(d$cases > 0, na.rm = TRUE)
     case_proportion <- round(case_weeks / n_observations * 100, 1)
     
     # Missing data assessment
     na_counts <- sapply(d, function(x) sum(is.na(x)))
     high_na_vars <- names(na_counts[na_counts > (nrow(d) * 0.1)])  # >10% missing
     high_na_count <- length(high_na_vars)
     
     # Variable categories count
     temporal_count <- length(temporal_cols)
     climate_count <- length(basic_climate_cols) + length(rolling_cols) + length(anomaly_cols) + 
                     length(extremes_cols) + length(indices_cols) + 
                     if(include_lags) length(lag_cols) else 0
     interaction_count <- length(interaction_cols) + length(enhanced_interaction_cols)
     
     # Print summary
     message(sprintf("Countries: %d (%s)", n_countries, countries_str))
     message(sprintf("Covariate period: %s to %s (%.1f years)", 
                    format(covariate_date_start, "%Y-%m-%d"), 
                    format(covariate_date_end, "%Y-%m-%d"), 
                    covariate_span_years))
     if (!is.na(case_date_start) && !is.na(case_date_end)) {
          message(sprintf("Observed case period: %s to %s (%.1f years)", 
                         format(case_date_start, "%Y-%m-%d"), 
                         format(case_date_end, "%Y-%m-%d"), 
                         case_span_years))
     } else {
          message("Observed case period: No non-zero cases found")
     }
     message(sprintf("Observations: %s total", format(n_observations, big.mark = ",")))
     message(sprintf("Cases: %s total cases, %s deaths", 
                    format(total_cases, big.mark = ","), 
                    format(total_deaths, big.mark = ",")))
     message(sprintf("Case weeks: %s (%.1f%% of observations)", 
                    format(case_weeks, big.mark = ","), case_proportion))
     message(sprintf("Covariates: %d total", n_covariates))
     message(sprintf("  - Temporal: %d", temporal_count))
     message(sprintf("  - Climate: %d", climate_count))
     message(sprintf("  - Demographics/Health: %d", length(demo_cols) + length(health_cols)))
     message(sprintf("  - WASH: %d", length(wash_cols)))
     message(sprintf("  - ENSO: %d", length(enso_cols)))
     message(sprintf("  - Interactions: %d", interaction_count))
     message(sprintf("  - Epidemic memory: %d", length(memory_cols)))
     spatial_count <- length(structural_spatial_cols[structural_spatial_cols %in% names(d)]) + 
                     length(case_based_spatial_cols[case_based_spatial_cols %in% names(d)])
     message(sprintf("  - Spatial import: %d (%d structural + %d case-based)", 
                    spatial_count, 
                    length(structural_spatial_cols[structural_spatial_cols %in% names(d)]),
                    length(case_based_spatial_cols[case_based_spatial_cols %in% names(d)])))
     message(sprintf("  - Time lags: %d", if(include_lags) length(lag_cols) else 0))
     message(sprintf("  - Geographic: %d", length(geo_cols)))
     
     if (high_na_count > 0) {
          message(sprintf("Variables with >10%% missing data: %d", high_na_count))
     }
     
     # Mode-specific messages
     if (forecast_mode) {
          if (forecast_horizon > 1) {
               message(sprintf("MODE: Forecast (%d-month horizon) - Epidemic memory with %d-week lag, case-based spatial variables with reliability warning", 
                              forecast_horizon, memory_lag))
          } else {
               message(sprintf("MODE: Forecast (%d-month horizon) - Epidemic memory with %d-week lag, all spatial variables included", 
                              forecast_horizon, memory_lag))
          }
     } else {
          message("MODE: Historical analysis - All variables included")
     }
     
     if (include_lags) {
          message("LAGS: Time-lagged climate variables included (1-36 week lags)")
     } else {
          message("LAGS: No time-lagged variables")
     }
     
     message("=======================")

     # Save the merged dataset to a CSV file
     path <- file.path(PATHS$DATA_CHOLERA_WEEKLY, 'cholera_country_weekly_suitability_data.csv')
     write.csv(d, file = path, row.names = FALSE)
     message("Processed suitability data saved here: ", path)
}
