% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calc_convergence_diagnostics.R
\name{calc_convergence_diagnostics}
\alias{calc_convergence_diagnostics}
\title{Calculate comprehensive convergence diagnostics with status indicators}
\usage{
calc_convergence_diagnostics(
  n_total,
  n_successful,
  n_retained,
  n_best_subset,
  ess_best,
  A_best,
  cvw_best,
  percentile_used,
  convergence_tier,
  param_ess_results = NULL,
  target_ess_best = 300,
  target_A_best = 0.95,
  target_cvw_best = 0.5,
  target_max_best_subset = 1000,
  target_ess_param = 300,
  target_ess_param_prop = 0.95,
  ess_method = c("kish", "perplexity"),
  temperature = 1,
  verbose = TRUE
)
}
\arguments{
\item{n_total}{Integer total number of simulations attempted}

\item{n_successful}{Integer number of simulations that completed successfully
(finite likelihood)}

\item{n_retained}{Integer number of simulations retained after outlier removal}

\item{n_best_subset}{Integer number of simulations in the optimized best subset}

\item{ess_best}{Numeric effective sample size within the best subset}

\item{A_best}{Numeric agreement index (entropy-based) for the best subset}

\item{cvw_best}{Numeric coefficient of variation of weights in the best subset}

\item{percentile_used}{Numeric percentile of likelihood distribution used for
best subset selection (e.g., 5.0 for top 5\%)}

\item{convergence_tier}{Character string indicating which tier criteria were
used (e.g., "tier_3", "fallback")}

\item{param_ess_results}{Optional data frame with columns \code{parameter} and
\code{ess_marginal} containing parameter-specific ESS values. If NULL,
parameter ESS metrics are omitted.}

\item{target_ess_best}{Numeric target for both subset size and ESS (default 300).
Both B_size and ESS_B must be >= target_ess_best.}

\item{target_A_best}{Numeric target agreement index (default 0.95)}

\item{target_cvw_best}{Numeric target coefficient of variation (default 0.5)}

\item{target_max_best_subset}{Numeric maximum best subset size in absolute count
(default 1000). Replaced target_percentile_max for absolute count-based control.}

\item{target_ess_param}{Numeric target ESS for individual parameters (default 300)}

\item{target_ess_param_prop}{Numeric target proportion of parameters that must
meet ESS threshold (default 0.95, meaning 95\%)}

\item{ess_method}{Character string specifying ESS calculation method: "kish" (default)
or "perplexity". This should match the method used for all ESS calculations in the
workflow.}

\item{temperature}{Numeric temperature parameter used for weight scaling
(default 1, for documentation purposes)}

\item{verbose}{Logical indicating whether to print diagnostic messages
(default TRUE)}
}
\value{
A list with the following structure:
\describe{
\item{settings}{List with optimization_tier, percentile_used, temperature, description}
\item{targets}{List of target values with descriptions for each metric}
\item{metrics}{List of metric values with descriptions and status indicators}
\item{summary}{List with counts, tier info, and overall convergence status}
}

The returned list is ready for JSON serialization and contains all information
needed for visualization by \code{\link{plot_model_convergence_status}}.
}
\description{
Centralized function for computing all convergence diagnostics and status
indicators for BFRS calibration results. Takes metrics and targets as input,
calculates pass/warn/fail status for each metric using consistent thresholds,
and aggregates to an overall convergence status.
}
\section{Status Thresholds}{

Status indicators are calculated using multiplicative thresholds relative to
targets:

\strong{For "higher is better" metrics (ESS, Agreement):}
\itemize{
\item \strong{PASS}: value >= target * pass_threshold
\item \strong{WARN}: value >= target * warn_threshold but < pass_threshold
\item \strong{FAIL}: value < target * warn_threshold
}

\strong{For "lower is better" metrics (CVw):}
\itemize{
\item \strong{PASS}: value <= target * pass_threshold
\item \strong{WARN}: value <= target * warn_threshold but > pass_threshold
\item \strong{FAIL}: value > target * warn_threshold
}

\strong{Default thresholds by metric:}
\itemize{
\item ESS_B: pass=0.8 (80\%), warn=0.5 (50\%)
\item A_B: pass=0.9 (90\%), warn=0.7 (70\%)
\item CVw_B: pass=1.2 (120\%), warn=2.0 (200\%)
\item B_size: pass=1.0 (100\%), warn=0.5 (50\%)
\item Percentile: pass=1.0 (100\%), warn=1.5 (150\%)
\item Param ESS: pass=1.0 (100\%), warn=0.8 (80\%)
}
}

\section{Overall Status}{

The overall convergence status is determined by aggregating individual metric
statuses:
\itemize{
\item \strong{FAIL}: Any metric has status "fail"
\item \strong{WARN}: No failures, but at least one metric has status "warn"
\item \strong{PASS}: All metrics have status "pass"
}
}

\examples{
\dontrun{
# After running BFRS calibration and calculating metrics
diagnostics <- calc_convergence_diagnostics(
    n_total = 10000,
    n_successful = 9500,
    n_retained = 8500,
    n_best_subset = 500,
    ess_best = 280,
    A_best = 0.92,
    cvw_best = 0.55,
    percentile_used = 4.8,
    convergence_tier = "tier_3",
    param_ess_results = param_ess_df,
    target_ess_best = 300,
    target_A_best = 0.95,
    target_cvw_best = 0.5,
    target_max_best_subset = 1000,
    ess_method = "kish"
)

# Save to JSON
jsonlite::write_json(diagnostics, "convergence_diagnostics.json",
                     pretty = TRUE, auto_unbox = TRUE)

# Check overall status
print(diagnostics$summary$convergence_status)
}

}
\seealso{
\code{\link{plot_model_convergence_status}},
\code{\link{calc_model_ess}},
\code{\link{calc_model_agreement_index}}

Other calibration-metrics: 
\code{\link{calc_model_agreement_index}()},
\code{\link{calc_model_aic_delta}()},
\code{\link{calc_model_akaike_weights}()},
\code{\link{calc_model_convergence}()},
\code{\link{calc_model_cvw}()},
\code{\link{calc_model_ess}()},
\code{\link{calc_model_max_weight}()},
\code{\link{plot_model_convergence}()}
}
\concept{calibration-metrics}
