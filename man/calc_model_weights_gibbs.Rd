% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calc_model_weights_gibbs.R
\name{calc_model_weights_gibbs}
\alias{calc_model_weights_gibbs}
\title{Gibbs-Posterior Model Weights}
\usage{
calc_model_weights_gibbs(x, temperature, verbose = FALSE)
}
\arguments{
\item{x}{Numeric vector of model scores (treated as loss-like; lower is better).}

\item{temperature}{Non-negative scalar inverse temperature \eqn{\eta}. Higher
values concentrate weight on the best models; lower values flatten weights.
Typical choices: \code{0.5} for \eqn{\Delta}AIC (Akaike weights),
\code{1} for \eqn{-\log L}.}

\item{verbose}{Logical; if \code{TRUE}, prints brief diagnostics (range of
\code{x}, applied shift, entropy, and effective number of models).}
}
\value{
A numeric vector of length \code{length(x)} containing weights that sum to 1.
Names are preserved when present.
}
\description{
Converts a numeric vector of model scores (treated as \strong{loss-like}, i.e.,
lower-is-better) into normalized posterior weights using the Gibbs
(generalized Bayes) formulation:
\deqn{w_m(\eta) = \frac{\exp(-\eta \, x_m)}{\sum_{j=1}^K \exp(-\eta \, x_j)} \,,}
where \eqn{x_m} is the loss (or information criterion) for model \eqn{m} and
\eqn{\eta \ge 0} is the inverse temperature (sharpness) parameter.
}
\details{
This function is agnostic to the specific form of \eqn{x}; smaller values are
assumed to indicate better models. Common choices include:
\itemize{
\item \eqn{x = \Delta \mathrm{AIC}} \; (use \code{temperature = 1/2} to
recover Akaike weights),
\item \eqn{x = -\log L} \; (use \code{temperature = 1} for normalized likelihood weights),
\item Cross-validated losses (e.g., RMSE, MSLE), or any proper scoring rule.
}
If you start from a higher-is-better score (e.g., log-likelihood), pass
\code{x = -score} so that lower-is-better holds.

Numerical stability: the implementation subtracts \eqn{\min(x)} prior to
exponentiation; this shift leaves the normalized weights unchanged but helps
avoid underflow/overflow.

Special cases:
\itemize{
\item \code{temperature = 0}: uniform weights.
\item \code{temperature = Inf}: hard selection (ties share weight equally).
\item All \code{x} equal: uniform weights.
}
}
\examples{
# Three models with Delta AIC = (0, 2, 6): Akaike weights via temperature = 0.5
calc_model_weights_gibbs(x = c(0, 2, 6), temperature = 0.5)

# From negative log-likelihoods: normalized likelihood weights with temperature = 1
calc_model_weights_gibbs(x = c(120.3, 121.1, 124.8), temperature = 1)

# Using a CV loss (lower is better) with moderate sharpness
set.seed(1)
losses <- c(A = 0.83, B = 0.81, C = 0.92)
calc_model_weights_gibbs(losses, temperature = 3, verbose = TRUE)

# Temperature extremes
calc_model_weights_gibbs(c(1, 2, 5), temperature = 0)      # uniform
calc_model_weights_gibbs(c(1, 2, 5), temperature = Inf)    # hard selection

}
\references{
Bissiri, P. G., Holmes, C. C., & Walker, S. G. (2016).
A general framework for updating belief distributions.
\emph{Journal of the Royal Statistical Society: Series B}, 78(5), 1103â€“1130.
\href{https://doi.org/10.1111/rssb.12158}{doi:10.1111/rssb.12158}
}
